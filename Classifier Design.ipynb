{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import graphviz\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (No need to use in case of pickle load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  (635580, 38)\n",
      "Backstepping_1 (the most agile): % 48.107712640422925\n",
      "Backstepping_2 (agile): % 1.7302306554643\n",
      "Backstepping_3 (smooth): % 6.893388715818623\n",
      "Backstepping_4 (the smoothest): % 43.268667988294155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>z0</th>\n",
       "      <th>x_dot0</th>\n",
       "      <th>y_dot0</th>\n",
       "      <th>z_dot0</th>\n",
       "      <th>phi0</th>\n",
       "      <th>theta0</th>\n",
       "      <th>yaw0</th>\n",
       "      <th>phi_dot0</th>\n",
       "      <th>...</th>\n",
       "      <th>yp</th>\n",
       "      <th>zp</th>\n",
       "      <th>x_dotp</th>\n",
       "      <th>y_dotp</th>\n",
       "      <th>z_dotp</th>\n",
       "      <th>x_ddotp</th>\n",
       "      <th>y_ddotp</th>\n",
       "      <th>z_ddotp</th>\n",
       "      <th>u_abs_p</th>\n",
       "      <th>controller_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.900659</td>\n",
       "      <td>0.280235</td>\n",
       "      <td>1.696418</td>\n",
       "      <td>-4.579493</td>\n",
       "      <td>2.070832</td>\n",
       "      <td>-3.238523</td>\n",
       "      <td>-0.145760</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>-0.728182</td>\n",
       "      <td>0.235309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280235</td>\n",
       "      <td>1.696418</td>\n",
       "      <td>-4.579493</td>\n",
       "      <td>2.070832</td>\n",
       "      <td>-3.238523</td>\n",
       "      <td>2.387939</td>\n",
       "      <td>-1.887509</td>\n",
       "      <td>1.637147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.905235</td>\n",
       "      <td>0.282304</td>\n",
       "      <td>1.693181</td>\n",
       "      <td>-4.572434</td>\n",
       "      <td>2.067247</td>\n",
       "      <td>-3.236889</td>\n",
       "      <td>-0.145536</td>\n",
       "      <td>0.589948</td>\n",
       "      <td>-0.727442</td>\n",
       "      <td>0.212167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280235</td>\n",
       "      <td>1.696418</td>\n",
       "      <td>-4.579493</td>\n",
       "      <td>2.070832</td>\n",
       "      <td>-3.238523</td>\n",
       "      <td>2.387939</td>\n",
       "      <td>-1.887509</td>\n",
       "      <td>1.637147</td>\n",
       "      <td>209.736982</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.909804</td>\n",
       "      <td>0.284370</td>\n",
       "      <td>1.689944</td>\n",
       "      <td>-4.565362</td>\n",
       "      <td>2.063656</td>\n",
       "      <td>-3.235251</td>\n",
       "      <td>-0.145261</td>\n",
       "      <td>0.590609</td>\n",
       "      <td>-0.726654</td>\n",
       "      <td>0.338972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282305</td>\n",
       "      <td>1.693181</td>\n",
       "      <td>-4.577105</td>\n",
       "      <td>2.068945</td>\n",
       "      <td>-3.236885</td>\n",
       "      <td>2.387934</td>\n",
       "      <td>-1.885310</td>\n",
       "      <td>1.639824</td>\n",
       "      <td>337.163728</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.914366</td>\n",
       "      <td>0.286432</td>\n",
       "      <td>1.686710</td>\n",
       "      <td>-4.558282</td>\n",
       "      <td>2.060063</td>\n",
       "      <td>-3.233609</td>\n",
       "      <td>-0.144858</td>\n",
       "      <td>0.590787</td>\n",
       "      <td>-0.725777</td>\n",
       "      <td>0.466631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284373</td>\n",
       "      <td>1.689944</td>\n",
       "      <td>-4.574717</td>\n",
       "      <td>2.067061</td>\n",
       "      <td>-3.235244</td>\n",
       "      <td>2.387929</td>\n",
       "      <td>-1.883113</td>\n",
       "      <td>1.642499</td>\n",
       "      <td>321.673448</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.918920</td>\n",
       "      <td>0.288490</td>\n",
       "      <td>1.683477</td>\n",
       "      <td>-4.551201</td>\n",
       "      <td>2.056470</td>\n",
       "      <td>-3.231963</td>\n",
       "      <td>-0.144328</td>\n",
       "      <td>0.590509</td>\n",
       "      <td>-0.724811</td>\n",
       "      <td>0.594331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286439</td>\n",
       "      <td>1.686710</td>\n",
       "      <td>-4.572329</td>\n",
       "      <td>2.065179</td>\n",
       "      <td>-3.233600</td>\n",
       "      <td>2.387924</td>\n",
       "      <td>-1.880916</td>\n",
       "      <td>1.645171</td>\n",
       "      <td>306.518919</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.923468</td>\n",
       "      <td>0.290545</td>\n",
       "      <td>1.680246</td>\n",
       "      <td>-4.544126</td>\n",
       "      <td>2.052881</td>\n",
       "      <td>-3.230312</td>\n",
       "      <td>-0.143671</td>\n",
       "      <td>0.589799</td>\n",
       "      <td>-0.723761</td>\n",
       "      <td>0.721346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288504</td>\n",
       "      <td>1.683477</td>\n",
       "      <td>-4.569941</td>\n",
       "      <td>2.063299</td>\n",
       "      <td>-3.231953</td>\n",
       "      <td>2.387918</td>\n",
       "      <td>-1.878721</td>\n",
       "      <td>1.647841</td>\n",
       "      <td>291.675474</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.928009</td>\n",
       "      <td>0.292596</td>\n",
       "      <td>1.677017</td>\n",
       "      <td>-4.537063</td>\n",
       "      <td>2.049301</td>\n",
       "      <td>-3.228656</td>\n",
       "      <td>-0.142887</td>\n",
       "      <td>0.588683</td>\n",
       "      <td>-0.722627</td>\n",
       "      <td>0.847045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290566</td>\n",
       "      <td>1.680246</td>\n",
       "      <td>-4.567553</td>\n",
       "      <td>2.061421</td>\n",
       "      <td>-3.230304</td>\n",
       "      <td>2.387911</td>\n",
       "      <td>-1.876527</td>\n",
       "      <td>1.650509</td>\n",
       "      <td>277.184081</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.932542</td>\n",
       "      <td>0.294643</td>\n",
       "      <td>1.673789</td>\n",
       "      <td>-4.530018</td>\n",
       "      <td>2.045731</td>\n",
       "      <td>-3.226997</td>\n",
       "      <td>-0.141979</td>\n",
       "      <td>0.587185</td>\n",
       "      <td>-0.721412</td>\n",
       "      <td>0.970884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292626</td>\n",
       "      <td>1.677017</td>\n",
       "      <td>-4.565166</td>\n",
       "      <td>2.059546</td>\n",
       "      <td>-3.228652</td>\n",
       "      <td>2.387904</td>\n",
       "      <td>-1.874335</td>\n",
       "      <td>1.653174</td>\n",
       "      <td>263.078594</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.937069</td>\n",
       "      <td>0.296687</td>\n",
       "      <td>1.670563</td>\n",
       "      <td>-4.522998</td>\n",
       "      <td>2.042176</td>\n",
       "      <td>-3.225333</td>\n",
       "      <td>-0.140947</td>\n",
       "      <td>0.585328</td>\n",
       "      <td>-0.720119</td>\n",
       "      <td>1.092399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294685</td>\n",
       "      <td>1.673789</td>\n",
       "      <td>-4.562778</td>\n",
       "      <td>2.057673</td>\n",
       "      <td>-3.226998</td>\n",
       "      <td>2.387897</td>\n",
       "      <td>-1.872143</td>\n",
       "      <td>1.655837</td>\n",
       "      <td>249.386166</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.941588</td>\n",
       "      <td>0.298728</td>\n",
       "      <td>1.667338</td>\n",
       "      <td>-4.516008</td>\n",
       "      <td>2.038637</td>\n",
       "      <td>-3.223666</td>\n",
       "      <td>-0.139796</td>\n",
       "      <td>0.583133</td>\n",
       "      <td>-0.718749</td>\n",
       "      <td>1.211204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296742</td>\n",
       "      <td>1.670563</td>\n",
       "      <td>-4.560390</td>\n",
       "      <td>2.055802</td>\n",
       "      <td>-3.225341</td>\n",
       "      <td>2.387889</td>\n",
       "      <td>-1.869953</td>\n",
       "      <td>1.658497</td>\n",
       "      <td>236.127719</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        y0        z0    x_dot0    y_dot0    z_dot0      phi0  \\\n",
       "0 -1.900659  0.280235  1.696418 -4.579493  2.070832 -3.238523 -0.145760   \n",
       "1 -1.905235  0.282304  1.693181 -4.572434  2.067247 -3.236889 -0.145536   \n",
       "2 -1.909804  0.284370  1.689944 -4.565362  2.063656 -3.235251 -0.145261   \n",
       "3 -1.914366  0.286432  1.686710 -4.558282  2.060063 -3.233609 -0.144858   \n",
       "4 -1.918920  0.288490  1.683477 -4.551201  2.056470 -3.231963 -0.144328   \n",
       "5 -1.923468  0.290545  1.680246 -4.544126  2.052881 -3.230312 -0.143671   \n",
       "6 -1.928009  0.292596  1.677017 -4.537063  2.049301 -3.228656 -0.142887   \n",
       "7 -1.932542  0.294643  1.673789 -4.530018  2.045731 -3.226997 -0.141979   \n",
       "8 -1.937069  0.296687  1.670563 -4.522998  2.042176 -3.225333 -0.140947   \n",
       "9 -1.941588  0.298728  1.667338 -4.516008  2.038637 -3.223666 -0.139796   \n",
       "\n",
       "     theta0      yaw0  phi_dot0  ...        yp        zp    x_dotp    y_dotp  \\\n",
       "0  0.589219 -0.728182  0.235309  ...  0.280235  1.696418 -4.579493  2.070832   \n",
       "1  0.589948 -0.727442  0.212167  ...  0.280235  1.696418 -4.579493  2.070832   \n",
       "2  0.590609 -0.726654  0.338972  ...  0.282305  1.693181 -4.577105  2.068945   \n",
       "3  0.590787 -0.725777  0.466631  ...  0.284373  1.689944 -4.574717  2.067061   \n",
       "4  0.590509 -0.724811  0.594331  ...  0.286439  1.686710 -4.572329  2.065179   \n",
       "5  0.589799 -0.723761  0.721346  ...  0.288504  1.683477 -4.569941  2.063299   \n",
       "6  0.588683 -0.722627  0.847045  ...  0.290566  1.680246 -4.567553  2.061421   \n",
       "7  0.587185 -0.721412  0.970884  ...  0.292626  1.677017 -4.565166  2.059546   \n",
       "8  0.585328 -0.720119  1.092399  ...  0.294685  1.673789 -4.562778  2.057673   \n",
       "9  0.583133 -0.718749  1.211204  ...  0.296742  1.670563 -4.560390  2.055802   \n",
       "\n",
       "     z_dotp   x_ddotp   y_ddotp   z_ddotp     u_abs_p   controller_ID  \n",
       "0 -3.238523  2.387939 -1.887509  1.637147    1.000000  Backstepping_4  \n",
       "1 -3.238523  2.387939 -1.887509  1.637147  209.736982  Backstepping_1  \n",
       "2 -3.236885  2.387934 -1.885310  1.639824  337.163728  Backstepping_1  \n",
       "3 -3.235244  2.387929 -1.883113  1.642499  321.673448  Backstepping_1  \n",
       "4 -3.233600  2.387924 -1.880916  1.645171  306.518919  Backstepping_1  \n",
       "5 -3.231953  2.387918 -1.878721  1.647841  291.675474  Backstepping_1  \n",
       "6 -3.230304  2.387911 -1.876527  1.650509  277.184081  Backstepping_1  \n",
       "7 -3.228652  2.387904 -1.874335  1.653174  263.078594  Backstepping_1  \n",
       "8 -3.226998  2.387897 -1.872143  1.655837  249.386166  Backstepping_1  \n",
       "9 -3.225341  2.387889 -1.869953  1.658497  236.127719  Backstepping_1  \n",
       "\n",
       "[10 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('flight.csv')\n",
    "# data2 = pd.read_csv('dataset/flight_littleone.csv')\n",
    "# data3 = pd.read_csv('dataset/flight_mustafa.csv')\n",
    "# data4 = pd.read_csv('dataset/flight_ubuntu.csv')\n",
    "# frames = [data1, data2, data3, data4]\n",
    "# result = pd.concat(frames)\n",
    "N_B1 = np.sum(data[\"controller_ID\"] == \"Backstepping_1\")\n",
    "N_B2 = np.sum(data[\"controller_ID\"] == \"Backstepping_2\")\n",
    "N_B3 = np.sum(data[\"controller_ID\"] == \"Backstepping_3\")\n",
    "N_B4 = np.sum(data[\"controller_ID\"] == \"Backstepping_4\")\n",
    "\n",
    "print(\"Dataset size: \", data.shape)\n",
    "print(\"Backstepping_1 (the most agile): %\", N_B1/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_2 (agile): %\", N_B2/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_3 (smooth): %\", N_B3/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_4 (the smoothest): %\", N_B4/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  (484993, 37)\n",
      "X_val size:  (85587, 37)\n",
      "X_test size:  (65000, 37)\n"
     ]
    }
   ],
   "source": [
    "N_Test = 65000\n",
    "dataset = data.values\n",
    "controller_labels = {'Backstepping_1': 0, 'Backstepping_2': 1, 'Backstepping_3': 2, 'Backstepping_4': 3}\n",
    "np.random.shuffle(dataset)\n",
    "y = np.array([controller_labels[data[-1]] for data in dataset]).reshape(-1,)\n",
    "X = dataset[:,:-1]\n",
    "\n",
    "X_test = X[0:N_Test,:]\n",
    "y_test = y[0:N_Test]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[N_Test:,:], y[N_Test:], test_size=0.15, random_state=42)\n",
    "\n",
    "# Saving the objects:\n",
    "# with open('dataset.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([X_train, X_val, X_test, y_train, y_val, y_test], f)\n",
    "    \n",
    "pickle.dump([X_train, X_val, X_test, y_train, y_val, y_test], open(\"dataset.pkl\",\"wb\"), protocol=2)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print (\"X_train size: \",X_train.shape)\n",
    "print (\"X_val size: \",X_val.shape)\n",
    "print (\"X_test size: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain size:  (1326141, 37)\n",
      "Xval size:  (234026, 37)\n",
      "Xtest size:  (165000, 37)\n"
     ]
    }
   ],
   "source": [
    "with open('dataset.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(f)\n",
    "\n",
    "with open('dataset_mustafa.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X_trainM, X_valM, X_testM, y_trainM, y_valM, y_testM = pickle.load(f)\n",
    "    \n",
    "Xtrain = np.concatenate((X_train, X_trainM), axis=0)\n",
    "Xval = np.concatenate((X_val, X_valM), axis=0)\n",
    "Xtest = np.concatenate((X_test, X_testM), axis=0)\n",
    "\n",
    "ytrain = np.concatenate((y_train, y_trainM), axis=0)\n",
    "yval = np.concatenate((y_val, y_valM), axis=0)\n",
    "ytest = np.concatenate((y_test, y_testM), axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xval = scaler.transform(Xval)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "print (\"Xtrain size: \",Xtrain.shape)\n",
    "print (\"Xval size: \",Xval.shape)\n",
    "print (\"Xtest size: \",Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Train Acc: 1.0, Val Acc: 0.996, Test Acc: 0.996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    if inner_tree.value[index].min() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "    # if there are shildren, visit them as well\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
    "\n",
    "\n",
    "feature_names = ['x0', 'y0', 'z0', 'x_dot0','y_dot0','z_dot0', 'phi0','theta0','yaw0', 'phi_dot0','theta_dot0','yaw_dot0', \n",
    "                 'xf', 'yf', 'zf', 'x_dotf','y_dotf','z_dotf','x_ddotf','y_ddotf','z_ddotf',\n",
    "                 'pos_diffx','pos_diffy','pos_diffz','time_rate','t', 'Tf', \n",
    "                 'xp', 'yp', 'zp', 'x_dotp','y_dotp','z_dotp','x_ddotp','y_ddotp','z_ddotp', 'u_abs_p']\n",
    "\n",
    "class_names = ['Backstepping_1', 'Backstepping_2', 'Backstepping_3', 'Backstepping_4']\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(Xtrain, ytrain)\n",
    "# prune_index(clf.tree_, 0, 10)\n",
    "\n",
    "train_pred = clf.predict(Xtrain)\n",
    "val_pred = clf.predict(Xval)\n",
    "test_pred = clf.predict(Xtest)\n",
    "\n",
    "\n",
    "train_acc = accuracy_score(ytrain, train_pred)\n",
    "val_acc = accuracy_score(yval, val_pred)\n",
    "test_acc = accuracy_score(ytest, test_pred)\n",
    "\n",
    "print (\"Decision Tree Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))\n",
    "\n",
    "\n",
    "# forest = RandomForestClassifier(bootstrap=True, n_estimators=100, \n",
    "#                                 min_samples_split=12, min_samples_leaf=5, max_features=10, max_depth=80)\n",
    "# forest = forest.fit(X_train, y_train)\n",
    "\n",
    "# train_pred = forest.predict(X_train)\n",
    "# val_pred = forest.predict(X_val)\n",
    "# test_pred = forest.predict(X_test)\n",
    "\n",
    "\n",
    "# train_acc = accuracy_score(y_train, train_pred)\n",
    "# val_acc = accuracy_score(y_val, val_pred)\n",
    "# test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "# print (\"Random Forest Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))\n",
    "\n",
    "# dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "#                       feature_names=feature_names,  \n",
    "#                       class_names=class_names,  \n",
    "#                       filled=True, rounded=True,  \n",
    "#                       special_characters=True)\n",
    "# graph = graphviz.Source(dot_data) \n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "stats_filename = 'params_results.csv'\n",
    "\n",
    "def write_results(results): \n",
    "    df_stats = pd.DataFrame([results], columns=stats_columns)\n",
    "    df_stats.to_csv(stats_filename, mode='a', index=False,header=not os.path.isfile(stats_filename))\n",
    "\n",
    "def predict(X, y, model):\n",
    "    #Validation part\n",
    "    model.eval()  # Set model to training mode\n",
    "\n",
    "    inputs, labels = torch.from_numpy(X).to(device), torch.from_numpy(y).to(device).long()\n",
    "\n",
    "    outputs = model(inputs.float())\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    accuracy = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "    \n",
    "    print (\"Test data, Loss: {0:.3}, Accuracy: {1:.4}\".format(loss.item(), accuracy))\n",
    "    \n",
    "\n",
    "def shuffle_dataset(X, y):\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], y[p]\n",
    "\n",
    "def train_model(X, y, X_val, y_val, model, criterion, optimizer, scheduler, minibatch_size, num_epochs=25):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracy_train = []\n",
    "    accuracy_val = []\n",
    "    # path = F\"/content/drive/My Drive/best_model.pt\"\n",
    "#     directory = path_name\n",
    "\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        X_train, y_train = shuffle_dataset(X, y)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        losses_iter = []\n",
    "        accuracy_iter = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i in range(0, X_train.shape[0], minibatch_size):\n",
    "            # Get pair of (X, y) of the current minibatch/chunk             \n",
    "            X_batch = X_train[i:i + minibatch_size]\n",
    "            y_batch = y_train[i:i + minibatch_size]\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = torch.from_numpy(X_batch).to(device), torch.from_numpy(y_batch).to(device).long()\n",
    "\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs.float())\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # print (\"losses_iter\", loss.item() * inputs.size(0))\n",
    "            # print (\"accuracy_iter\", torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "\n",
    "            losses_iter.append(loss.item())\n",
    "            accuracy_iter.append(torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(losses_iter)\n",
    "        train_acc = np.mean(accuracy_iter)\n",
    "\n",
    "        losses_train.append(train_loss)\n",
    "        accuracy_train.append(train_acc)\n",
    "\n",
    "\n",
    "        print('Training Loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc))\n",
    "        \n",
    "        #Validation part\n",
    "        model.eval()  # Set model to training mode\n",
    "        \n",
    "        inputs, labels = torch.from_numpy(X_val).to(device), torch.from_numpy(y_val).to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss = loss.item()\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        val_acc = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "        \n",
    "        losses_val.append(val_loss)\n",
    "        accuracy_val.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print('Validation Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc))\n",
    "\n",
    "#         deep copy the model\n",
    "        if val_acc > best_acc:\n",
    "            best_train_acc = train_acc\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            path = \"best_model.pt\"\n",
    "            torch.save(best_model_wts, path)\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}s'.format(time_elapsed))\n",
    "    print('Best Train Acc: {:4f}'.format(best_train_acc))\n",
    "    print('Best Val Acc: {:4f}'.format(best_val_acc))\n",
    "    \n",
    "    \n",
    "#     stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "    layers = [module for module in model.modules() if type(module) != nn.Sequential]\n",
    "    write_results([layers, num_epochs, minibatch_size, learning_rate, optimizer.state_dict, scheduler.state_dict, best_train_acc, best_val_acc])\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Training Loss: 0.3261 Acc: 0.8816\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 268.00 MiB (GPU 0; 4.00 GiB total capacity; 2.82 GiB already allocated; 257.26 MiB free; 15.74 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c34d1a528f12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mfinal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-164427dfefe5>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X, y, X_val, y_val, model, criterion, optimizer, scheduler, minibatch_size, num_epochs)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\merkez\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c34d1a528f12>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\merkez\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m    911\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 913\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 268.00 MiB (GPU 0; 4.00 GiB total capacity; 2.82 GiB already allocated; 257.26 MiB free; 15.74 MiB cached)"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop_layer = nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(37, 800)\n",
    "        self.fc2 = nn.Linear(800, 600)\n",
    "        self.fc3 = nn.Linear(600, 300)\n",
    "        self.fc4 = nn.Linear(300, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "n_epochs = 10\n",
    "minibatch_size = 128\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "final_model = train_model(Xtrain, ytrain, Xval, yval, model, criterion, optimizer, scheduler, minibatch_size, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data, Loss: 0.138, Accuracy: 0.9471\n"
     ]
    }
   ],
   "source": [
    "predict(X_test, y_test, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1326141, 37)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainM.shape\n",
    "\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "with open('dataset_mustafa.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X_trainM, X_valM, X_testM, y_trainM, y_valM, y_testM = pickle.load(f)\n",
    "    \n",
    "scalerM = StandardScaler()\n",
    "scalerM.fit(X_trainM)\n",
    "\n",
    "X_trainM = scalerM.transform(X_trainM)\n",
    "X_valM = scalerM.transform(X_valM)\n",
    "X_testM = scalerM.transform(X_testM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data, Loss: 0.957, Accuracy: 0.7829\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "predict(X_valM, y_valM, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest, Val Acc: 0.443, Test Acc: 0.444\n"
     ]
    }
   ],
   "source": [
    "val_predM = forest.predict(X_valM)\n",
    "test_predM = forest.predict(X_testM)\n",
    "\n",
    "val_accM = accuracy_score(y_valM, val_predM)\n",
    "test_accM = accuracy_score(y_testM, test_predM)\n",
    "\n",
    "print (\"Random Forest, Val Acc: {0:.3}, Test Acc: {1:.3}\".format(val_accM, test_accM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = svm.SVC(kernel='rbf')\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# svc = svc.fit(X_train, y_train)\n",
    "\n",
    "# train_pred = svc.predict(X_train)\n",
    "# val_pred = svc.predict(X_val)\n",
    "# test_pred = svc.predict(X_test)\n",
    "\n",
    "\n",
    "# train_acc = accuracy_score(y_train, train_pred)\n",
    "# val_acc = accuracy_score(y_val, val_pred)\n",
    "# test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "# print (\"SVM Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
