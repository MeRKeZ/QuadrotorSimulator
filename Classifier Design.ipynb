{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import graphviz\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (No need to use in case of pickle load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv('flight.csv')\n",
    "# data2 = pd.read_csv('flight_neu.csv')\n",
    "# data3 = pd.read_csv('flight_wn.csv')\n",
    "# # data3 = pd.read_csv('dataset/flight_mustafa.csv')\n",
    "# # data4 = pd.read_csv('dataset/flight_ubuntu.csv')\n",
    "# frames = [data1, data2, data3]#, data4]\n",
    "# data = pd.concat(frames)\n",
    "# N_B1 = np.sum(data[\"controller_ID\"] == \"Backstepping_1\")\n",
    "# N_B2 = np.sum(data[\"controller_ID\"] == \"Backstepping_2\")\n",
    "# N_B3 = np.sum(data[\"controller_ID\"] == \"Backstepping_3\")\n",
    "# N_B4 = np.sum(data[\"controller_ID\"] == \"Backstepping_4\")\n",
    "\n",
    "# print(\"Dataset size: \", data.shape)\n",
    "# print(\"Backstepping_1 (the most agile): %\", N_B1/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "# print(\"Backstepping_2 (agile): %\", N_B2/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "# print(\"Backstepping_3 (smooth): %\", N_B3/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "# print(\"Backstepping_4 (the smoothest): %\", N_B4/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  (76292, 38)\n",
      "Backstepping_1 (the most agile): % 33.78598018140828\n",
      "Backstepping_2 (agile): % 3.076338279242909\n",
      "Backstepping_3 (smooth): % 33.60902846956431\n",
      "Backstepping_4 (the smoothest): % 29.52865306978451\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>z0</th>\n",
       "      <th>x_dot0</th>\n",
       "      <th>y_dot0</th>\n",
       "      <th>z_dot0</th>\n",
       "      <th>phi0</th>\n",
       "      <th>theta0</th>\n",
       "      <th>yaw0</th>\n",
       "      <th>phi_dot0</th>\n",
       "      <th>...</th>\n",
       "      <th>yp</th>\n",
       "      <th>zp</th>\n",
       "      <th>x_dotp</th>\n",
       "      <th>y_dotp</th>\n",
       "      <th>z_dotp</th>\n",
       "      <th>x_ddotp</th>\n",
       "      <th>y_ddotp</th>\n",
       "      <th>z_ddotp</th>\n",
       "      <th>u_abs_p</th>\n",
       "      <th>controller_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.850561</td>\n",
       "      <td>1.359698</td>\n",
       "      <td>4.642320</td>\n",
       "      <td>-0.582726</td>\n",
       "      <td>-0.762233</td>\n",
       "      <td>1.538706</td>\n",
       "      <td>-0.915301</td>\n",
       "      <td>0.180243</td>\n",
       "      <td>-1.502666</td>\n",
       "      <td>0.449425</td>\n",
       "      <td>...</td>\n",
       "      <td>1.360460</td>\n",
       "      <td>4.640781</td>\n",
       "      <td>-0.595883</td>\n",
       "      <td>-0.761163</td>\n",
       "      <td>1.546005</td>\n",
       "      <td>1.280145</td>\n",
       "      <td>-0.520413</td>\n",
       "      <td>0.632062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.841616</td>\n",
       "      <td>1.344309</td>\n",
       "      <td>4.673153</td>\n",
       "      <td>-0.324584</td>\n",
       "      <td>-0.773924</td>\n",
       "      <td>1.549486</td>\n",
       "      <td>-0.771842</td>\n",
       "      <td>0.167495</td>\n",
       "      <td>-1.451927</td>\n",
       "      <td>10.473490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345901</td>\n",
       "      <td>4.670269</td>\n",
       "      <td>-0.572248</td>\n",
       "      <td>-0.771392</td>\n",
       "      <td>1.557965</td>\n",
       "      <td>1.207893</td>\n",
       "      <td>-0.556212</td>\n",
       "      <td>0.626819</td>\n",
       "      <td>115.216348</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.836987</td>\n",
       "      <td>1.328816</td>\n",
       "      <td>4.704266</td>\n",
       "      <td>-0.151595</td>\n",
       "      <td>-0.775964</td>\n",
       "      <td>1.567250</td>\n",
       "      <td>-0.542689</td>\n",
       "      <td>0.157239</td>\n",
       "      <td>-1.351921</td>\n",
       "      <td>12.101720</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330360</td>\n",
       "      <td>4.701553</td>\n",
       "      <td>-0.548841</td>\n",
       "      <td>-0.782887</td>\n",
       "      <td>1.570446</td>\n",
       "      <td>1.133008</td>\n",
       "      <td>-0.593112</td>\n",
       "      <td>0.621245</td>\n",
       "      <td>98.965678</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.835076</td>\n",
       "      <td>1.313213</td>\n",
       "      <td>4.735675</td>\n",
       "      <td>-0.049155</td>\n",
       "      <td>-0.784552</td>\n",
       "      <td>1.569280</td>\n",
       "      <td>-0.296302</td>\n",
       "      <td>0.155773</td>\n",
       "      <td>-1.226957</td>\n",
       "      <td>12.394616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.314581</td>\n",
       "      <td>4.733086</td>\n",
       "      <td>-0.526920</td>\n",
       "      <td>-0.795112</td>\n",
       "      <td>1.582814</td>\n",
       "      <td>1.059316</td>\n",
       "      <td>-0.629213</td>\n",
       "      <td>0.615614</td>\n",
       "      <td>143.696512</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.834670</td>\n",
       "      <td>1.297282</td>\n",
       "      <td>4.767187</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.807563</td>\n",
       "      <td>1.581925</td>\n",
       "      <td>-0.053117</td>\n",
       "      <td>0.143138</td>\n",
       "      <td>-1.092890</td>\n",
       "      <td>11.735078</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298551</td>\n",
       "      <td>4.764865</td>\n",
       "      <td>-0.506460</td>\n",
       "      <td>-0.808050</td>\n",
       "      <td>1.595070</td>\n",
       "      <td>0.986810</td>\n",
       "      <td>-0.664521</td>\n",
       "      <td>0.609928</td>\n",
       "      <td>201.414022</td>\n",
       "      <td>Backstepping_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.834775</td>\n",
       "      <td>1.280825</td>\n",
       "      <td>4.798924</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>-0.833805</td>\n",
       "      <td>1.607577</td>\n",
       "      <td>0.159959</td>\n",
       "      <td>0.092304</td>\n",
       "      <td>-0.959613</td>\n",
       "      <td>8.793254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282254</td>\n",
       "      <td>4.796888</td>\n",
       "      <td>-0.487439</td>\n",
       "      <td>-0.821687</td>\n",
       "      <td>1.607211</td>\n",
       "      <td>0.915484</td>\n",
       "      <td>-0.699041</td>\n",
       "      <td>0.604187</td>\n",
       "      <td>141.531459</td>\n",
       "      <td>Backstepping_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.834578</td>\n",
       "      <td>1.263716</td>\n",
       "      <td>4.831460</td>\n",
       "      <td>-0.027346</td>\n",
       "      <td>-0.875931</td>\n",
       "      <td>1.652156</td>\n",
       "      <td>0.292072</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>-0.830722</td>\n",
       "      <td>4.549058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.265679</td>\n",
       "      <td>4.829153</td>\n",
       "      <td>-0.469833</td>\n",
       "      <td>-0.836007</td>\n",
       "      <td>1.619237</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>-0.732779</td>\n",
       "      <td>0.598393</td>\n",
       "      <td>94.206820</td>\n",
       "      <td>Backstepping_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.833550</td>\n",
       "      <td>1.245749</td>\n",
       "      <td>4.864548</td>\n",
       "      <td>-0.077373</td>\n",
       "      <td>-0.913907</td>\n",
       "      <td>1.672903</td>\n",
       "      <td>0.349325</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.706248</td>\n",
       "      <td>1.394529</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248810</td>\n",
       "      <td>4.861657</td>\n",
       "      <td>-0.453619</td>\n",
       "      <td>-0.850993</td>\n",
       "      <td>1.631146</td>\n",
       "      <td>0.776341</td>\n",
       "      <td>-0.765739</td>\n",
       "      <td>0.592545</td>\n",
       "      <td>90.532910</td>\n",
       "      <td>Backstepping_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.831458</td>\n",
       "      <td>1.226965</td>\n",
       "      <td>4.898019</td>\n",
       "      <td>-0.132104</td>\n",
       "      <td>-0.955076</td>\n",
       "      <td>1.678926</td>\n",
       "      <td>0.355552</td>\n",
       "      <td>-0.075333</td>\n",
       "      <td>-0.586113</td>\n",
       "      <td>-0.608326</td>\n",
       "      <td>...</td>\n",
       "      <td>1.231635</td>\n",
       "      <td>4.894398</td>\n",
       "      <td>-0.438772</td>\n",
       "      <td>-0.866631</td>\n",
       "      <td>1.642938</td>\n",
       "      <td>0.708510</td>\n",
       "      <td>-0.797927</td>\n",
       "      <td>0.586644</td>\n",
       "      <td>76.831918</td>\n",
       "      <td>Backstepping_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.828265</td>\n",
       "      <td>1.207211</td>\n",
       "      <td>4.931433</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-1.015663</td>\n",
       "      <td>1.652699</td>\n",
       "      <td>0.330217</td>\n",
       "      <td>-0.102366</td>\n",
       "      <td>-0.470233</td>\n",
       "      <td>-1.822644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214140</td>\n",
       "      <td>4.927374</td>\n",
       "      <td>-0.425270</td>\n",
       "      <td>-0.882905</td>\n",
       "      <td>1.654612</td>\n",
       "      <td>0.641831</td>\n",
       "      <td>-0.829347</td>\n",
       "      <td>0.580691</td>\n",
       "      <td>62.791172</td>\n",
       "      <td>Backstepping_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        y0        z0    x_dot0    y_dot0    z_dot0      phi0  \\\n",
       "0  1.850561  1.359698  4.642320 -0.582726 -0.762233  1.538706 -0.915301   \n",
       "1  1.841616  1.344309  4.673153 -0.324584 -0.773924  1.549486 -0.771842   \n",
       "2  1.836987  1.328816  4.704266 -0.151595 -0.775964  1.567250 -0.542689   \n",
       "3  1.835076  1.313213  4.735675 -0.049155 -0.784552  1.569280 -0.296302   \n",
       "4  1.834670  1.297282  4.767187  0.000199 -0.807563  1.581925 -0.053117   \n",
       "5  1.834775  1.280825  4.798924  0.003299 -0.833805  1.607577  0.159959   \n",
       "6  1.834578  1.263716  4.831460 -0.027346 -0.875931  1.652156  0.292072   \n",
       "7  1.833550  1.245749  4.864548 -0.077373 -0.913907  1.672903  0.349325   \n",
       "8  1.831458  1.226965  4.898019 -0.132104 -0.955076  1.678926  0.355552   \n",
       "9  1.828265  1.207211  4.931433 -0.186892 -1.015663  1.652699  0.330217   \n",
       "\n",
       "     theta0      yaw0   phi_dot0  ...        yp        zp    x_dotp    y_dotp  \\\n",
       "0  0.180243 -1.502666   0.449425  ...  1.360460  4.640781 -0.595883 -0.761163   \n",
       "1  0.167495 -1.451927  10.473490  ...  1.345901  4.670269 -0.572248 -0.771392   \n",
       "2  0.157239 -1.351921  12.101720  ...  1.330360  4.701553 -0.548841 -0.782887   \n",
       "3  0.155773 -1.226957  12.394616  ...  1.314581  4.733086 -0.526920 -0.795112   \n",
       "4  0.143138 -1.092890  11.735078  ...  1.298551  4.764865 -0.506460 -0.808050   \n",
       "5  0.092304 -0.959613   8.793254  ...  1.282254  4.796888 -0.487439 -0.821687   \n",
       "6  0.027469 -0.830722   4.549058  ...  1.265679  4.829153 -0.469833 -0.836007   \n",
       "7 -0.032035 -0.706248   1.394529  ...  1.248810  4.861657 -0.453619 -0.850993   \n",
       "8 -0.075333 -0.586113  -0.608326  ...  1.231635  4.894398 -0.438772 -0.866631   \n",
       "9 -0.102366 -0.470233  -1.822644  ...  1.214140  4.927374 -0.425270 -0.882905   \n",
       "\n",
       "     z_dotp   x_ddotp   y_ddotp   z_ddotp     u_abs_p   controller_ID  \n",
       "0  1.546005  1.280145 -0.520413  0.632062    1.000000  Backstepping_4  \n",
       "1  1.557965  1.207893 -0.556212  0.626819  115.216348  Backstepping_1  \n",
       "2  1.570446  1.133008 -0.593112  0.621245   98.965678  Backstepping_1  \n",
       "3  1.582814  1.059316 -0.629213  0.615614  143.696512  Backstepping_1  \n",
       "4  1.595070  0.986810 -0.664521  0.609928  201.414022  Backstepping_1  \n",
       "5  1.607211  0.915484 -0.699041  0.604187  141.531459  Backstepping_3  \n",
       "6  1.619237  0.845330 -0.732779  0.598393   94.206820  Backstepping_3  \n",
       "7  1.631146  0.776341 -0.765739  0.592545   90.532910  Backstepping_3  \n",
       "8  1.642938  0.708510 -0.797927  0.586644   76.831918  Backstepping_3  \n",
       "9  1.654612  0.641831 -0.829347  0.580691   62.791172  Backstepping_3  \n",
       "\n",
       "[10 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('flight_gauss.csv')\n",
    "# data2 = pd.read_csv('flight_neu.csv')\n",
    "# data3 = pd.read_csv('flight.csv')\n",
    "# data4 = pd.read_csv('flight_wn.csv')\n",
    "frames = [data1]#, data2, data3, data4]\n",
    "data = pd.concat(frames)\n",
    "N_B1 = np.sum(data[\"controller_ID\"] == \"Backstepping_1\")\n",
    "N_B2 = np.sum(data[\"controller_ID\"] == \"Backstepping_2\")\n",
    "N_B3 = np.sum(data[\"controller_ID\"] == \"Backstepping_3\")\n",
    "N_B4 = np.sum(data[\"controller_ID\"] == \"Backstepping_4\")\n",
    "\n",
    "print(\"Dataset size: \", data.shape)\n",
    "print(\"Backstepping_1 (the most agile): %\", N_B1/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_2 (agile): %\", N_B2/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_3 (smooth): %\", N_B3/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_4 (the smoothest): %\", N_B4/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  (64162, 37)\n",
      "X_val size:  (7130, 37)\n",
      "X_test size:  (5000, 37)\n"
     ]
    }
   ],
   "source": [
    "N_Test = 5000\n",
    "dataset = data.values\n",
    "controller_labels = {'Backstepping_1': 0, 'Backstepping_2': 1, 'Backstepping_3': 2, 'Backstepping_4': 3}\n",
    "np.random.shuffle(dataset)\n",
    "y = np.array([controller_labels[data[-1]] for data in dataset]).reshape(-1,)\n",
    "X = dataset[:,:-1]\n",
    "\n",
    "X_test = X[0:N_Test,:]\n",
    "y_test = y[0:N_Test]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[N_Test:,:], y[N_Test:], test_size=0.1, random_state=42)\n",
    "\n",
    "# Saving the objects:\n",
    "# with open('dataset.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([X_train, X_val, X_test, y_train, y_val, y_test], f)\n",
    "    \n",
    "pickle.dump([X_train, X_val, X_test, y_train, y_val, y_test], open(\"dataset.pkl\",\"wb\"), protocol=2)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print (\"X_train size: \",X_train.shape)\n",
    "print (\"X_val size: \",X_val.shape)\n",
    "print (\"X_test size: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain size:  (64162, 37)\n",
      "Xval size:  (7130, 37)\n",
      "Xtest size:  (5000, 37)\n"
     ]
    }
   ],
   "source": [
    "with open('dataset.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(f)\n",
    "\n",
    "#with open('dataset_mustafa.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "#    X_trainM, X_valM, X_testM, y_trainM, y_valM, y_testM = pickle.load(f)\n",
    "    \n",
    "#Xtrain = np.concatenate((X_train, X_trainM), axis=0)\n",
    "#Xval = np.concatenate((X_val, X_valM), axis=0)\n",
    "#Xtest = np.concatenate((X_test, X_testM), axis=0)\n",
    "\n",
    "#ytrain = np.concatenate((y_train, y_trainM), axis=0)\n",
    "#yval = np.concatenate((y_val, y_valM), axis=0)\n",
    "#ytest = np.concatenate((y_test, y_testM), axis=0)\n",
    "\n",
    "Xtrain = X_train\n",
    "Xval = X_val\n",
    "Xtest = X_test\n",
    "ytrain = y_train\n",
    "yval = y_val\n",
    "ytest = y_test\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xval = scaler.transform(Xval)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "print (\"Xtrain size: \",Xtrain.shape)\n",
    "print (\"Xval size: \",Xval.shape)\n",
    "print (\"Xtest size: \",Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Train Acc: 1.0, Val Acc: 0.627, Test Acc: 0.609\n",
      "Random Forest Train Acc: 0.911, Val Acc: 0.728, Test Acc: 0.717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    if inner_tree.value[index].min() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "    # if there are shildren, visit them as well\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
    "\n",
    "\n",
    "feature_names = ['x0', 'y0', 'z0', 'x_dot0','y_dot0','z_dot0', 'phi0','theta0','yaw0', 'phi_dot0','theta_dot0','yaw_dot0', \n",
    "                 'xf', 'yf', 'zf', 'x_dotf','y_dotf','z_dotf','x_ddotf','y_ddotf','z_ddotf',\n",
    "                 'pos_diffx','pos_diffy','pos_diffz','time_rate','t', 'Tf', \n",
    "                 'xp', 'yp', 'zp', 'x_dotp','y_dotp','z_dotp','x_ddotp','y_ddotp','z_ddotp', 'u_abs_p', 'wind_direct']\n",
    "\n",
    "class_names = ['Backstepping_1', 'Backstepping_2', 'Backstepping_3', 'Backstepping_4']\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(Xtrain, ytrain)\n",
    "# prune_index(clf.tree_, 0, 10)\n",
    "\n",
    "train_pred = clf.predict(Xtrain)\n",
    "val_pred = clf.predict(Xval)\n",
    "test_pred = clf.predict(Xtest)\n",
    "\n",
    "\n",
    "train_acc = accuracy_score(ytrain, train_pred)\n",
    "val_acc = accuracy_score(yval, val_pred)\n",
    "test_acc = accuracy_score(ytest, test_pred)\n",
    "\n",
    "print (\"Decision Tree Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap=True, n_estimators=100, \n",
    "                                min_samples_split=12, min_samples_leaf=5, max_features=10, max_depth=80)\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "train_pred = forest.predict(X_train)\n",
    "val_pred = forest.predict(X_val)\n",
    "test_pred = forest.predict(X_test)\n",
    "\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print (\"Random Forest Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))\n",
    "\n",
    "# dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "#                       feature_names=feature_names,  \n",
    "#                       class_names=class_names,  \n",
    "#                       filled=True, rounded=True,  \n",
    "#                       special_characters=True)\n",
    "# graph = graphviz.Source(dot_data) \n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "stats_filename = 'params_results.csv'\n",
    "\n",
    "def write_results(results): \n",
    "    df_stats = pd.DataFrame([results], columns=stats_columns)\n",
    "    df_stats.to_csv(stats_filename, mode='a', index=False,header=not os.path.isfile(stats_filename))\n",
    "\n",
    "def predict(X, y, model):\n",
    "    #Validation part\n",
    "    model.eval()  # Set model to training mode\n",
    "    \n",
    "    inputs, labels = torch.from_numpy(X).to(device), torch.from_numpy(y).to(device).long()\n",
    "\n",
    "    outputs = model(inputs.float())\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    accuracy = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "    \n",
    "    print (\"Test data, Loss: {0:.3}, Accuracy: {1:.4}\".format(loss.item(), accuracy))\n",
    "    \n",
    "\n",
    "def shuffle_dataset(X, y):\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], y[p]\n",
    "\n",
    "def train_model(X, y, X_val, y_val, model, criterion, optimizer, scheduler, minibatch_size, num_epochs=25):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracy_train = []\n",
    "    accuracy_val = []\n",
    "    # path = F\"/content/drive/My Drive/best_model.pt\"\n",
    "#     directory = path_name\n",
    "\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        X_train, y_train = shuffle_dataset(X, y)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        losses_iter = []\n",
    "        accuracy_iter = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i in range(0, X_train.shape[0], minibatch_size):\n",
    "            # Get pair of (X, y) of the current minibatch/chunk             \n",
    "            X_batch = X_train[i:i + minibatch_size]\n",
    "            y_batch = y_train[i:i + minibatch_size]\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = torch.from_numpy(X_batch).to(device), torch.from_numpy(y_batch).to(device).long()\n",
    "\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs.float())\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # print (\"losses_iter\", loss.item() * inputs.size(0))\n",
    "            # print (\"accuracy_iter\", torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "\n",
    "            losses_iter.append(loss.item())\n",
    "            accuracy_iter.append(torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(losses_iter)\n",
    "        train_acc = np.mean(accuracy_iter)\n",
    "\n",
    "        losses_train.append(train_loss)\n",
    "        accuracy_train.append(train_acc)\n",
    "\n",
    "\n",
    "        print('Training Loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc))\n",
    "        \n",
    "        #Validation part\n",
    "        model.eval()  # Set model to training mode\n",
    "        \n",
    "        inputs, labels = torch.from_numpy(X_val).to(device), torch.from_numpy(y_val).to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss = loss.item()\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        val_acc = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "        \n",
    "        losses_val.append(val_loss)\n",
    "        accuracy_val.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print('Validation Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc))\n",
    "\n",
    "#         deep copy the model\n",
    "        if val_acc > best_acc:\n",
    "            best_train_acc = train_acc\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            path = \"best_model.pt\"\n",
    "            torch.save(best_model_wts, path)\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}s'.format(time_elapsed))\n",
    "    print('Best Train Acc: {:4f}'.format(best_train_acc))\n",
    "    print('Best Val Acc: {:4f}'.format(best_val_acc))\n",
    "    \n",
    "    \n",
    "#     stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "    layers = [module for module in model.modules() if type(module) != nn.Sequential]\n",
    "    #write_results([layers, num_epochs, minibatch_size, learning_rate, optimizer.state_dict, scheduler.state_dict, best_train_acc, best_val_acc])\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "Training Loss: 1.1416 Acc: 0.4292\n",
      "Validation Loss: 1.1191 Acc: 0.4546\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "Training Loss: 1.1023 Acc: 0.4622\n",
      "Validation Loss: 1.1095 Acc: 0.4523\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "Training Loss: 1.0757 Acc: 0.4835\n",
      "Validation Loss: 1.0657 Acc: 0.4987\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "Training Loss: 1.0330 Acc: 0.5215\n",
      "Validation Loss: 1.0625 Acc: 0.5205\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "Training Loss: 1.0122 Acc: 0.5390\n",
      "Validation Loss: 1.0540 Acc: 0.5180\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "Training Loss: 0.9987 Acc: 0.5449\n",
      "Validation Loss: 1.0091 Acc: 0.5449\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "Training Loss: 0.9900 Acc: 0.5495\n",
      "Validation Loss: 1.0142 Acc: 0.5362\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "Training Loss: 0.9841 Acc: 0.5534\n",
      "Validation Loss: 1.0085 Acc: 0.5440\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "Training Loss: 0.9759 Acc: 0.5582\n",
      "Validation Loss: 1.0031 Acc: 0.5478\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "Training Loss: 0.9714 Acc: 0.5620\n",
      "Validation Loss: 0.9887 Acc: 0.5537\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "Training Loss: 0.9656 Acc: 0.5632\n",
      "Validation Loss: 0.9872 Acc: 0.5492\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "Training Loss: 0.9601 Acc: 0.5668\n",
      "Validation Loss: 0.9935 Acc: 0.5574\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "Training Loss: 0.9545 Acc: 0.5675\n",
      "Validation Loss: 0.9846 Acc: 0.5593\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "Training Loss: 0.9515 Acc: 0.5733\n",
      "Validation Loss: 0.9767 Acc: 0.5663\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "Training Loss: 0.9472 Acc: 0.5730\n",
      "Validation Loss: 0.9737 Acc: 0.5571\n",
      "\n",
      "Training complete in 59s\n",
      "Best Train Acc: 0.573015\n",
      "Best Val Acc: 0.557083\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop_layer = nn.Dropout(p=0.0)\n",
    "        self.fc1 = nn.Linear(37, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "n_epochs = 15\n",
    "minibatch_size = 32\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "final_model = train_model(Xtrain, ytrain, Xval, yval, model, criterion, optimizer, scheduler, minibatch_size, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data, Loss: 0.971, Accuracy: 0.5502\n"
     ]
    }
   ],
   "source": [
    "predict(Xtest, ytest, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# with open('dataset_mustafa.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "#     X_trainM, X_valM, X_testM, y_trainM, y_valM, y_testM = pickle.load(f)\n",
    "    \n",
    "# scalerM = StandardScaler()\n",
    "# scalerM.fit(X_trainM)\n",
    "\n",
    "# X_trainM = scalerM.transform(X_trainM)\n",
    "# X_valM = scalerM.transform(X_valM)\n",
    "# X_testM = scalerM.transform(X_testM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# predict(X_valM, y_valM, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_predM = forest.predict(X_valM)\n",
    "# test_predM = forest.predict(X_testM)\n",
    "\n",
    "# val_accM = accuracy_score(y_valM, val_predM)\n",
    "# test_accM = accuracy_score(y_testM, test_predM)\n",
    "\n",
    "# print (\"Random Forest, Val Acc: {0:.3}, Test Acc: {1:.3}\".format(val_accM, test_accM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = svm.SVC(kernel='rbf')\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# svc = svc.fit(Xtrain, ytrain)\n",
    "\n",
    "# train_pred = svc.predict(Xtrain)\n",
    "# val_pred = svc.predict(Xval)\n",
    "# test_pred = svc.predict(X_est)\n",
    "\n",
    "\n",
    "# train_acc = accuracy_score(ytrain, train_pred)\n",
    "# val_acc = accuracy_score(yval, val_pred)\n",
    "# test_acc = accuracy_score(ytest, test_pred)\n",
    "\n",
    "# print (\"SVM Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
