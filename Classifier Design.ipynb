{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import graphviz\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (No need to use in case of pickle load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  (11035, 30)\n",
      "Backstepping_1 (the most agile): % 7.16810149524241\n",
      "Backstepping_2 (agile): % 47.449025826914365\n",
      "Backstepping_3 (smooth): % 16.683280471227913\n",
      "Backstepping_4 (the smoothest): % 28.699592206615314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_diffx</th>\n",
       "      <th>pos_diffy</th>\n",
       "      <th>pos_diffz</th>\n",
       "      <th>x_dot0</th>\n",
       "      <th>y_dot0</th>\n",
       "      <th>z_dot0</th>\n",
       "      <th>x_ddot0</th>\n",
       "      <th>y_ddot0</th>\n",
       "      <th>z_ddot0</th>\n",
       "      <th>phi0</th>\n",
       "      <th>...</th>\n",
       "      <th>z_dot</th>\n",
       "      <th>phi</th>\n",
       "      <th>theta</th>\n",
       "      <th>yaw</th>\n",
       "      <th>phi_dot</th>\n",
       "      <th>theta_dot</th>\n",
       "      <th>yaw_dot</th>\n",
       "      <th>Tf</th>\n",
       "      <th>Cost</th>\n",
       "      <th>controller_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.101286</td>\n",
       "      <td>2.977686</td>\n",
       "      <td>-3.909193</td>\n",
       "      <td>2.468151</td>\n",
       "      <td>0.058794</td>\n",
       "      <td>4.272227</td>\n",
       "      <td>0.418144</td>\n",
       "      <td>0.518302</td>\n",
       "      <td>2.407532</td>\n",
       "      <td>-0.120893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.000689</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-1.225973e-09</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>-2.031073e-08</td>\n",
       "      <td>7.582028</td>\n",
       "      <td>1694.665079</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.695773</td>\n",
       "      <td>10.494593</td>\n",
       "      <td>3.510018</td>\n",
       "      <td>-0.920061</td>\n",
       "      <td>4.401198</td>\n",
       "      <td>4.668143</td>\n",
       "      <td>-0.506166</td>\n",
       "      <td>1.392277</td>\n",
       "      <td>2.129325</td>\n",
       "      <td>0.438414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>-0.005825</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-1.104446e-08</td>\n",
       "      <td>0.385308</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>-6.457531e-08</td>\n",
       "      <td>7.503483</td>\n",
       "      <td>1321.615094</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-12.390705</td>\n",
       "      <td>5.088863</td>\n",
       "      <td>-12.357094</td>\n",
       "      <td>-2.131357</td>\n",
       "      <td>4.191148</td>\n",
       "      <td>1.138735</td>\n",
       "      <td>-0.125261</td>\n",
       "      <td>-1.337830</td>\n",
       "      <td>-0.444336</td>\n",
       "      <td>-0.164007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>-9.221207e-08</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.321581</td>\n",
       "      <td>-1.900057e-08</td>\n",
       "      <td>7.103790</td>\n",
       "      <td>1052.200839</td>\n",
       "      <td>Backstepping_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.387150</td>\n",
       "      <td>0.400347</td>\n",
       "      <td>-15.443461</td>\n",
       "      <td>-1.718199</td>\n",
       "      <td>3.056684</td>\n",
       "      <td>-2.369332</td>\n",
       "      <td>1.743786</td>\n",
       "      <td>-2.298170</td>\n",
       "      <td>1.302001</td>\n",
       "      <td>-1.050622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>6.636610e-10</td>\n",
       "      <td>0.013054</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>1.824617e-10</td>\n",
       "      <td>9.375474</td>\n",
       "      <td>1511.047907</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-10.539222</td>\n",
       "      <td>-6.876072</td>\n",
       "      <td>-3.598480</td>\n",
       "      <td>0.278103</td>\n",
       "      <td>2.966335</td>\n",
       "      <td>4.034239</td>\n",
       "      <td>0.600516</td>\n",
       "      <td>-2.297271</td>\n",
       "      <td>-1.244859</td>\n",
       "      <td>-0.571294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>1.221864e-08</td>\n",
       "      <td>-0.045258</td>\n",
       "      <td>0.057145</td>\n",
       "      <td>1.731866e-08</td>\n",
       "      <td>9.093389</td>\n",
       "      <td>935.231718</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-12.508175</td>\n",
       "      <td>-7.813309</td>\n",
       "      <td>-4.425320</td>\n",
       "      <td>-0.054532</td>\n",
       "      <td>4.163760</td>\n",
       "      <td>3.291256</td>\n",
       "      <td>1.819945</td>\n",
       "      <td>-0.554834</td>\n",
       "      <td>-1.125834</td>\n",
       "      <td>-0.373333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-8.018437e-09</td>\n",
       "      <td>0.045062</td>\n",
       "      <td>0.116227</td>\n",
       "      <td>-1.194734e-08</td>\n",
       "      <td>7.394873</td>\n",
       "      <td>605.248002</td>\n",
       "      <td>Backstepping_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-11.186689</td>\n",
       "      <td>6.199794</td>\n",
       "      <td>-0.907260</td>\n",
       "      <td>1.834735</td>\n",
       "      <td>1.762263</td>\n",
       "      <td>0.835011</td>\n",
       "      <td>-2.017607</td>\n",
       "      <td>2.050102</td>\n",
       "      <td>-1.599122</td>\n",
       "      <td>-0.126951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.000826</td>\n",
       "      <td>-3.291682e-07</td>\n",
       "      <td>0.334794</td>\n",
       "      <td>0.235380</td>\n",
       "      <td>-9.488759e-07</td>\n",
       "      <td>6.539962</td>\n",
       "      <td>827.431466</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.281355</td>\n",
       "      <td>-2.623850</td>\n",
       "      <td>-5.374649</td>\n",
       "      <td>1.717146</td>\n",
       "      <td>3.715987</td>\n",
       "      <td>-1.141216</td>\n",
       "      <td>-0.378486</td>\n",
       "      <td>1.244190</td>\n",
       "      <td>-0.680990</td>\n",
       "      <td>-1.219954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>6.041105e-08</td>\n",
       "      <td>0.131167</td>\n",
       "      <td>-0.097695</td>\n",
       "      <td>5.160115e-08</td>\n",
       "      <td>9.094292</td>\n",
       "      <td>1104.910478</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.505178</td>\n",
       "      <td>4.777527</td>\n",
       "      <td>-7.538253</td>\n",
       "      <td>1.234812</td>\n",
       "      <td>-2.961899</td>\n",
       "      <td>3.700720</td>\n",
       "      <td>0.905495</td>\n",
       "      <td>0.453853</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>0.005010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>-4.034163e-08</td>\n",
       "      <td>-0.041214</td>\n",
       "      <td>-0.513032</td>\n",
       "      <td>-9.873421e-08</td>\n",
       "      <td>5.738680</td>\n",
       "      <td>295.089765</td>\n",
       "      <td>Backstepping_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-7.836972</td>\n",
       "      <td>15.847110</td>\n",
       "      <td>-7.417657</td>\n",
       "      <td>-1.342635</td>\n",
       "      <td>-3.089010</td>\n",
       "      <td>4.175744</td>\n",
       "      <td>1.596773</td>\n",
       "      <td>1.573064</td>\n",
       "      <td>0.286534</td>\n",
       "      <td>-0.916619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>-1.003906e-07</td>\n",
       "      <td>0.175869</td>\n",
       "      <td>0.120235</td>\n",
       "      <td>-1.869410e-07</td>\n",
       "      <td>7.246066</td>\n",
       "      <td>399.541089</td>\n",
       "      <td>Backstepping_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos_diffx  pos_diffy  pos_diffz    x_dot0    y_dot0    z_dot0   x_ddot0  \\\n",
       "0 -10.101286   2.977686  -3.909193  2.468151  0.058794  4.272227  0.418144   \n",
       "1   3.695773  10.494593   3.510018 -0.920061  4.401198  4.668143 -0.506166   \n",
       "2 -12.390705   5.088863 -12.357094 -2.131357  4.191148  1.138735 -0.125261   \n",
       "3  -0.387150   0.400347 -15.443461 -1.718199  3.056684 -2.369332  1.743786   \n",
       "4 -10.539222  -6.876072  -3.598480  0.278103  2.966335  4.034239  0.600516   \n",
       "5 -12.508175  -7.813309  -4.425320 -0.054532  4.163760  3.291256  1.819945   \n",
       "6 -11.186689   6.199794  -0.907260  1.834735  1.762263  0.835011 -2.017607   \n",
       "7   7.281355  -2.623850  -5.374649  1.717146  3.715987 -1.141216 -0.378486   \n",
       "8  11.505178   4.777527  -7.538253  1.234812 -2.961899  3.700720  0.905495   \n",
       "9  -7.836972  15.847110  -7.417657 -1.342635 -3.089010  4.175744  1.596773   \n",
       "\n",
       "    y_ddot0   z_ddot0      phi0  ...     z_dot       phi     theta  \\\n",
       "0  0.518302  2.407532 -0.120893  ...  0.000098 -0.000689 -0.000105   \n",
       "1  1.392277  2.129325  0.438414  ...  0.000140 -0.005825 -0.000159   \n",
       "2 -1.337830 -0.444336 -0.164007  ... -0.000019  0.000197  0.000286   \n",
       "3 -2.298170  1.302001 -1.050622  ... -0.000041  0.000078  0.000014   \n",
       "4 -2.297271 -1.244859 -0.571294  ...  0.000016  0.000236 -0.000196   \n",
       "5 -0.554834 -1.125834 -0.373333  ...  0.000002 -0.000131 -0.000214   \n",
       "6  2.050102 -1.599122 -0.126951  ... -0.000024 -0.001446 -0.000826   \n",
       "7  1.244190 -0.680990 -1.219954  ... -0.000036  0.000132 -0.000161   \n",
       "8  0.453853 -0.234571  0.005010  ... -0.000003  0.000173  0.001446   \n",
       "9  1.573064  0.286534 -0.916619  ...  0.000042 -0.001117 -0.000658   \n",
       "\n",
       "            yaw   phi_dot  theta_dot       yaw_dot        Tf         Cost  \\\n",
       "0 -1.225973e-09  0.063684   0.019662 -2.031073e-08  7.582028  1694.665079   \n",
       "1 -1.104446e-08  0.385308   0.006642 -6.457531e-08  7.503483  1321.615094   \n",
       "2 -9.221207e-08  0.233586   0.321581 -1.900057e-08  7.103790  1052.200839   \n",
       "3  6.636610e-10  0.013054  -0.006269  1.824617e-10  9.375474  1511.047907   \n",
       "4  1.221864e-08 -0.045258   0.057145  1.731866e-08  9.093389   935.231718   \n",
       "5 -8.018437e-09  0.045062   0.116227 -1.194734e-08  7.394873   605.248002   \n",
       "6 -3.291682e-07  0.334794   0.235380 -9.488759e-07  6.539962   827.431466   \n",
       "7  6.041105e-08  0.131167  -0.097695  5.160115e-08  9.094292  1104.910478   \n",
       "8 -4.034163e-08 -0.041214  -0.513032 -9.873421e-08  5.738680   295.089765   \n",
       "9 -1.003906e-07  0.175869   0.120235 -1.869410e-07  7.246066   399.541089   \n",
       "\n",
       "    controller_ID  \n",
       "0  Backstepping_4  \n",
       "1  Backstepping_4  \n",
       "2  Backstepping_2  \n",
       "3  Backstepping_4  \n",
       "4  Backstepping_4  \n",
       "5  Backstepping_2  \n",
       "6  Backstepping_4  \n",
       "7  Backstepping_4  \n",
       "8  Backstepping_2  \n",
       "9  Backstepping_4  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('dataset/flight.csv')\n",
    "data2 = pd.read_csv('dataset/flight_littleone.csv')\n",
    "data3 = pd.read_csv('dataset/flight_mustafa.csv')\n",
    "data4 = pd.read_csv('dataset/flight_ubuntu.csv')\n",
    "frames = [data1, data2, data3, data4]\n",
    "result = pd.concat(frames)\n",
    "N_B1 = np.sum(result[\"controller_ID\"] == \"Backstepping_1\")\n",
    "N_B2 = np.sum(result[\"controller_ID\"] == \"Backstepping_2\")\n",
    "N_B3 = np.sum(result[\"controller_ID\"] == \"Backstepping_3\")\n",
    "N_B4 = np.sum(result[\"controller_ID\"] == \"Backstepping_4\")\n",
    "\n",
    "print(\"Dataset size: \", result.shape)\n",
    "print(\"Backstepping_1 (the most agile): %\", N_B1/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_2 (agile): %\", N_B2/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_3 (smooth): %\", N_B3/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "print(\"Backstepping_4 (the smoothest): %\", N_B4/(N_B1+N_B2+N_B3+N_B4)*100)\n",
    "result[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Test = 1000\n",
    "dataset = result.values\n",
    "controller_labels = {'Backstepping_1': 0, 'Backstepping_2': 1, 'Backstepping_3': 2, 'Backstepping_4': 3}\n",
    "np.random.shuffle(dataset)\n",
    "y = np.array([controller_labels[data[-1]] for data in dataset]).reshape(-1,)\n",
    "X = dataset[:,:-1]\n",
    "\n",
    "X_test = X[0:N_Test,:]\n",
    "y_test = y[0:N_Test]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[N_Test:,:], y[N_Test:], test_size=0.15, random_state=42)\n",
    "\n",
    "# Saving the objects:\n",
    "# with open('dataset.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([X_train, X_val, X_test, y_train, y_val, y_test], f)\n",
    "    \n",
    "pickle.dump([X_train, X_val, X_test, y_train, y_val, y_test], open(\"dataset.pkl\",\"wb\"), protocol=2)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print (\"X_train size: \",X_train.shape)\n",
    "print (\"X_val size: \",X_val.shape)\n",
    "print (\"X_test size: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Train Acc: 0.768, Val Acc: 0.748, Test Acc: 0.768\n",
      "Random Forest Train Acc: 0.987, Val Acc: 0.95, Test Acc: 0.952\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"585pt\" height=\"790pt\"\r\n",
       " viewBox=\"0.00 0.00 585.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-786 581,-786 581,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#d0f8cc\" stroke=\"black\" d=\"M388.5,-782C388.5,-782 193.5,-782 193.5,-782 187.5,-782 181.5,-776 181.5,-770 181.5,-770 181.5,-711 181.5,-711 181.5,-705 187.5,-699 193.5,-699 193.5,-699 388.5,-699 388.5,-699 394.5,-699 400.5,-705 400.5,-711 400.5,-711 400.5,-770 400.5,-770 400.5,-776 394.5,-782 388.5,-782\"/>\r\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">yaw_dot ≤ &#45;0.006</text>\r\n",
       "<text text-anchor=\"start\" x=\"253.5\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.661</text>\r\n",
       "<text text-anchor=\"start\" x=\"239.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8529</text>\r\n",
       "<text text-anchor=\"start\" x=\"189.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [608, 4019, 1433, 2469]</text>\r\n",
       "<text text-anchor=\"start\" x=\"215\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_2</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#c8f7c4\" stroke=\"black\" d=\"M280.5,-663C280.5,-663 93.5,-663 93.5,-663 87.5,-663 81.5,-657 81.5,-651 81.5,-651 81.5,-592 81.5,-592 81.5,-586 87.5,-580 93.5,-580 93.5,-580 280.5,-580 280.5,-580 286.5,-580 292.5,-586 292.5,-592 292.5,-592 292.5,-651 292.5,-651 292.5,-657 286.5,-663 280.5,-663\"/>\r\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">yaw_dot ≤ &#45;0.006</text>\r\n",
       "<text text-anchor=\"start\" x=\"153.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.61</text>\r\n",
       "<text text-anchor=\"start\" x=\"135.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 7660</text>\r\n",
       "<text text-anchor=\"start\" x=\"89.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [483, 4018, 704, 2455]</text>\r\n",
       "<text text-anchor=\"start\" x=\"111\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_2</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M254.918,-698.907C246.851,-689.832 238.219,-680.121 229.906,-670.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"232.278,-668.17 223.018,-663.021 227.046,-672.82 232.278,-668.17\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"221.55\" y=\"-684.278\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 964 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>964</title>\r\n",
       "<path fill=\"#5eafea\" stroke=\"black\" d=\"M469,-655.5C469,-655.5 323,-655.5 323,-655.5 317,-655.5 311,-649.5 311,-643.5 311,-643.5 311,-599.5 311,-599.5 311,-593.5 317,-587.5 323,-587.5 323,-587.5 469,-587.5 469,-587.5 475,-587.5 481,-593.5 481,-599.5 481,-599.5 481,-643.5 481,-643.5 481,-649.5 475,-655.5 469,-655.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"358.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.275</text>\r\n",
       "<text text-anchor=\"start\" x=\"348.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 869</text>\r\n",
       "<text text-anchor=\"start\" x=\"319\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [125, 1, 729, 14]</text>\r\n",
       "<text text-anchor=\"start\" x=\"320\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_3</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;964 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>0&#45;&gt;964</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.429,-698.907C337.827,-687.321 349.156,-674.698 359.555,-663.111\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.161,-665.447 366.235,-655.667 356.951,-660.772 362.161,-665.447\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"367.584\" y=\"-676.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#62b1ea\" stroke=\"black\" d=\"M158,-536.5C158,-536.5 12,-536.5 12,-536.5 6,-536.5 0,-530.5 0,-524.5 0,-524.5 0,-480.5 0,-480.5 0,-474.5 6,-468.5 12,-468.5 12,-468.5 158,-468.5 158,-468.5 164,-468.5 170,-474.5 170,-480.5 170,-480.5 170,-524.5 170,-524.5 170,-530.5 164,-536.5 158,-536.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"47.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.301</text>\r\n",
       "<text text-anchor=\"start\" x=\"37.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 772</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [112, 0, 635, 25]</text>\r\n",
       "<text text-anchor=\"start\" x=\"9\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_3</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.612,-579.907C141.607,-568.432 130.715,-555.938 120.693,-544.442\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"123.124,-541.905 113.915,-536.667 117.848,-546.505 123.124,-541.905\"/>\r\n",
       "</g>\r\n",
       "<!-- 19 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>19</title>\r\n",
       "<path fill=\"#bdf6b8\" stroke=\"black\" d=\"M379.5,-544C379.5,-544 200.5,-544 200.5,-544 194.5,-544 188.5,-538 188.5,-532 188.5,-532 188.5,-473 188.5,-473 188.5,-467 194.5,-461 200.5,-461 200.5,-461 379.5,-461 379.5,-461 385.5,-461 391.5,-467 391.5,-473 391.5,-473 391.5,-532 391.5,-532 391.5,-538 385.5,-544 379.5,-544\"/>\r\n",
       "<text text-anchor=\"start\" x=\"250.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">z_dot ≤ 0.01</text>\r\n",
       "<text text-anchor=\"start\" x=\"252.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.532</text>\r\n",
       "<text text-anchor=\"start\" x=\"238.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6888</text>\r\n",
       "<text text-anchor=\"start\" x=\"196.5\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [371, 4018, 69, 2430]</text>\r\n",
       "<text text-anchor=\"start\" x=\"214\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_2</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;19 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;19</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.735,-579.907C230.725,-570.832 239.274,-561.121 247.506,-551.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"250.347,-553.839 254.328,-544.021 245.093,-549.214 250.347,-553.839\"/>\r\n",
       "</g>\r\n",
       "<!-- 20 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>20</title>\r\n",
       "<path fill=\"#a6f29f\" stroke=\"black\" d=\"M277.5,-425C277.5,-425 98.5,-425 98.5,-425 92.5,-425 86.5,-419 86.5,-413 86.5,-413 86.5,-354 86.5,-354 86.5,-348 92.5,-342 98.5,-342 98.5,-342 277.5,-342 277.5,-342 283.5,-342 289.5,-348 289.5,-354 289.5,-354 289.5,-413 289.5,-413 289.5,-419 283.5,-425 277.5,-425\"/>\r\n",
       "<text text-anchor=\"start\" x=\"148.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">z_dot ≤ 0.01</text>\r\n",
       "<text text-anchor=\"start\" x=\"150.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.504</text>\r\n",
       "<text text-anchor=\"start\" x=\"136.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6161</text>\r\n",
       "<text text-anchor=\"start\" x=\"94.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [370, 3922, 48, 1821]</text>\r\n",
       "<text text-anchor=\"start\" x=\"112\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_2</text>\r\n",
       "</g>\r\n",
       "<!-- 19&#45;&gt;20 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>19&#45;&gt;20</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M254.612,-460.907C246.7,-451.832 238.234,-442.121 230.081,-432.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"232.535,-430.259 223.326,-425.021 227.259,-434.859 232.535,-430.259\"/>\r\n",
       "</g>\r\n",
       "<!-- 901 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>901</title>\r\n",
       "<path fill=\"#de5eea\" stroke=\"black\" d=\"M464,-417.5C464,-417.5 320,-417.5 320,-417.5 314,-417.5 308,-411.5 308,-405.5 308,-405.5 308,-361.5 308,-361.5 308,-355.5 314,-349.5 320,-349.5 320,-349.5 464,-349.5 464,-349.5 470,-349.5 476,-355.5 476,-361.5 476,-361.5 476,-405.5 476,-405.5 476,-411.5 470,-417.5 464,-417.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"358.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.28</text>\r\n",
       "<text text-anchor=\"start\" x=\"344.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 727</text>\r\n",
       "<text text-anchor=\"start\" x=\"319.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 96, 21, 609]</text>\r\n",
       "<text text-anchor=\"start\" x=\"316\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_4</text>\r\n",
       "</g>\r\n",
       "<!-- 19&#45;&gt;901 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>19&#45;&gt;901</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M325.388,-460.907C335.393,-449.432 346.285,-436.938 356.307,-425.442\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"359.152,-427.505 363.085,-417.667 353.876,-422.905 359.152,-427.505\"/>\r\n",
       "</g>\r\n",
       "<!-- 21 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>21</title>\r\n",
       "<path fill=\"#e88cf0\" stroke=\"black\" d=\"M161,-298.5C161,-298.5 15,-298.5 15,-298.5 9,-298.5 3,-292.5 3,-286.5 3,-286.5 3,-242.5 3,-242.5 3,-236.5 9,-230.5 15,-230.5 15,-230.5 161,-230.5 161,-230.5 167,-230.5 173,-236.5 173,-242.5 173,-242.5 173,-286.5 173,-286.5 173,-292.5 167,-298.5 161,-298.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"50.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.438</text>\r\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1225</text>\r\n",
       "<text text-anchor=\"start\" x=\"11\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 335, 35, 854]</text>\r\n",
       "<text text-anchor=\"start\" x=\"12\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_4</text>\r\n",
       "</g>\r\n",
       "<!-- 20&#45;&gt;21 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>20&#45;&gt;21</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.305,-341.907C143.497,-330.432 132.819,-317.938 122.993,-306.442\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.506,-303.995 116.348,-298.667 120.184,-308.543 125.506,-303.995\"/>\r\n",
       "</g>\r\n",
       "<!-- 126 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>126</title>\r\n",
       "<path fill=\"#86ee7c\" stroke=\"black\" d=\"M373,-306C373,-306 203,-306 203,-306 197,-306 191,-300 191,-294 191,-294 191,-235 191,-235 191,-229 197,-223 203,-223 203,-223 373,-223 373,-223 379,-223 385,-229 385,-235 385,-235 385,-294 385,-294 385,-300 379,-306 373,-306\"/>\r\n",
       "<text text-anchor=\"start\" x=\"240\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">theta0 ≤ &#45;1.426</text>\r\n",
       "<text text-anchor=\"start\" x=\"250.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.428</text>\r\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4936</text>\r\n",
       "<text text-anchor=\"start\" x=\"199\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [369, 3587, 13, 967]</text>\r\n",
       "<text text-anchor=\"start\" x=\"212\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_2</text>\r\n",
       "</g>\r\n",
       "<!-- 20&#45;&gt;126 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>20&#45;&gt;126</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.695,-341.907C230.451,-332.832 238.751,-323.121 246.744,-313.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"249.53,-315.897 253.367,-306.021 244.209,-311.349 249.53,-315.897\"/>\r\n",
       "</g>\r\n",
       "<!-- 127 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>127</title>\r\n",
       "<path fill=\"#ec9ff2\" stroke=\"black\" d=\"M260,-179.5C260,-179.5 116,-179.5 116,-179.5 110,-179.5 104,-173.5 104,-167.5 104,-167.5 104,-123.5 104,-123.5 104,-117.5 110,-111.5 116,-111.5 116,-111.5 260,-111.5 260,-111.5 266,-111.5 272,-117.5 272,-123.5 272,-123.5 272,-167.5 272,-167.5 272,-173.5 266,-179.5 260,-179.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"150.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.459</text>\r\n",
       "<text text-anchor=\"start\" x=\"140.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 233</text>\r\n",
       "<text text-anchor=\"start\" x=\"119.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 77, 3, 153]</text>\r\n",
       "<text text-anchor=\"start\" x=\"112\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_4</text>\r\n",
       "</g>\r\n",
       "<!-- 126&#45;&gt;127 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>126&#45;&gt;127</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M253.305,-222.907C243.497,-211.432 232.819,-198.938 222.993,-187.442\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"225.506,-184.995 216.348,-179.667 220.184,-189.543 225.506,-184.995\"/>\r\n",
       "</g>\r\n",
       "<!-- 182 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>182</title>\r\n",
       "<path fill=\"#7fed76\" stroke=\"black\" d=\"M472,-187C472,-187 302,-187 302,-187 296,-187 290,-181 290,-175 290,-175 290,-116 290,-116 290,-110 296,-104 302,-104 302,-104 472,-104 472,-104 478,-104 484,-110 484,-116 484,-116 484,-175 484,-175 484,-181 478,-187 472,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"345\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">phi0 ≤ &#45;1.345</text>\r\n",
       "<text text-anchor=\"start\" x=\"349.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.407</text>\r\n",
       "<text text-anchor=\"start\" x=\"335.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4703</text>\r\n",
       "<text text-anchor=\"start\" x=\"298\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [369, 3510, 10, 814]</text>\r\n",
       "<text text-anchor=\"start\" x=\"311\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_2</text>\r\n",
       "</g>\r\n",
       "<!-- 126&#45;&gt;182 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>126&#45;&gt;182</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M322.348,-222.907C330.026,-213.832 338.244,-204.121 346.157,-194.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"348.926,-196.916 352.713,-187.021 343.582,-192.394 348.926,-196.916\"/>\r\n",
       "</g>\r\n",
       "<!-- 183 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>183</title>\r\n",
       "<path fill=\"#efb2f5\" stroke=\"black\" d=\"M361,-68C361,-68 217,-68 217,-68 211,-68 205,-62 205,-56 205,-56 205,-12 205,-12 205,-6 211,-0 217,-0 217,-0 361,-0 361,-0 367,-0 373,-6 373,-12 373,-12 373,-56 373,-56 373,-62 367,-68 361,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"251.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.485</text>\r\n",
       "<text text-anchor=\"start\" x=\"241.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 262</text>\r\n",
       "<text text-anchor=\"start\" x=\"220.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [3, 97, 1, 161]</text>\r\n",
       "<text text-anchor=\"start\" x=\"213\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_4</text>\r\n",
       "</g>\r\n",
       "<!-- 182&#45;&gt;183 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>182&#45;&gt;183</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M350.508,-103.726C342.345,-94.6054 333.686,-84.93 325.522,-75.8078\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.079,-73.417 318.802,-68.2996 322.863,-78.0853 328.079,-73.417\"/>\r\n",
       "</g>\r\n",
       "<!-- 252 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>252</title>\r\n",
       "<path fill=\"#79ec6f\" stroke=\"black\" d=\"M565,-68C565,-68 403,-68 403,-68 397,-68 391,-62 391,-56 391,-56 391,-12 391,-12 391,-6 397,-0 403,-0 403,-0 565,-0 565,-0 571,-0 577,-6 577,-12 577,-12 577,-56 577,-56 577,-62 571,-68 565,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"446.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.381</text>\r\n",
       "<text text-anchor=\"start\" x=\"432.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4441</text>\r\n",
       "<text text-anchor=\"start\" x=\"399\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [366, 3413, 9, 653]</text>\r\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Backstepping_2</text>\r\n",
       "</g>\r\n",
       "<!-- 182&#45;&gt;252 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>182&#45;&gt;252</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M423.119,-103.726C431.199,-94.6054 439.77,-84.93 447.851,-75.8078\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"450.491,-78.1058 454.502,-68.2996 445.251,-73.4642 450.491,-78.1058\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x25c65f75f88>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    if inner_tree.value[index].min() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "    # if there are shildren, visit them as well\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
    "\n",
    "\n",
    "feature_names = ['pos_diffx','pos_diffy','pos_diffz', 'x_dot0','y_dot0','z_dot0', 'x_ddot0','y_ddot0','z_ddot0', 'phi0','theta0','yaw0', 'phi_dot0','theta_dot0','yaw_dot0', \n",
    "                 'x','y','z','x_dot','y_dot','z_dot', 'phi','theta','yaw', 'phi_dot','theta_dot','yaw_dot', 'Tf', 'Cost']\n",
    "\n",
    "class_names = ['Backstepping_1', 'Backstepping_2', 'Backstepping_3', 'Backstepping_4']\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "prune_index(clf.tree_, 0, 10)\n",
    "\n",
    "train_pred = clf.predict(X_train)\n",
    "val_pred = clf.predict(X_val)\n",
    "test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print (\"Decision Tree Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap=True, n_estimators=100, \n",
    "                                min_samples_split=12, min_samples_leaf=5, max_features=10, max_depth=80)\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "train_pred = forest.predict(X_train)\n",
    "val_pred = forest.predict(X_val)\n",
    "test_pred = forest.predict(X_test)\n",
    "\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print (\"Random Forest Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=feature_names,  \n",
    "                      class_names=class_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "stats_filename = 'params_results.csv'\n",
    "\n",
    "def write_results(results): \n",
    "    df_stats = pd.DataFrame([results], columns=stats_columns)\n",
    "    df_stats.to_csv(stats_filename, mode='a', index=False,header=not os.path.isfile(stats_filename))\n",
    "\n",
    "def predict(X, y, model):\n",
    "    #Validation part\n",
    "    model.eval()  # Set model to training mode\n",
    "\n",
    "    inputs, labels = torch.from_numpy(X).to(device), torch.from_numpy(y).to(device).long()\n",
    "\n",
    "    outputs = model(inputs.float())\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    accuracy = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "    \n",
    "    print (\"Test data, Loss: {0:.3}, Accuracy: {1:.4}\".format(loss.item(), accuracy))\n",
    "    \n",
    "\n",
    "def shuffle_dataset(X, y):\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], y[p]\n",
    "\n",
    "def train_model(X, y, X_val, y_val, model, criterion, optimizer, scheduler, minibatch_size, num_epochs=25):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracy_train = []\n",
    "    accuracy_val = []\n",
    "    # path = F\"/content/drive/My Drive/best_model.pt\"\n",
    "#     directory = path_name\n",
    "\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        X_train, y_train = shuffle_dataset(X, y)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        losses_iter = []\n",
    "        accuracy_iter = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i in range(0, X_train.shape[0], minibatch_size):\n",
    "            # Get pair of (X, y) of the current minibatch/chunk             \n",
    "            X_batch = X_train[i:i + minibatch_size]\n",
    "            y_batch = y_train[i:i + minibatch_size]\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = torch.from_numpy(X_batch).to(device), torch.from_numpy(y_batch).to(device).long()\n",
    "\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs.float())\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # print (\"losses_iter\", loss.item() * inputs.size(0))\n",
    "            # print (\"accuracy_iter\", torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "\n",
    "            losses_iter.append(loss.item())\n",
    "            accuracy_iter.append(torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(losses_iter)\n",
    "        train_acc = np.mean(accuracy_iter)\n",
    "\n",
    "        losses_train.append(train_loss)\n",
    "        accuracy_train.append(train_acc)\n",
    "\n",
    "\n",
    "        print('Training Loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc))\n",
    "        \n",
    "        #Validation part\n",
    "        model.eval()  # Set model to training mode\n",
    "        \n",
    "        inputs, labels = torch.from_numpy(X_val).to(device), torch.from_numpy(y_val).to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss = loss.item()\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        val_acc = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "        \n",
    "        losses_val.append(val_loss)\n",
    "        accuracy_val.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print('Validation Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc))\n",
    "\n",
    "#         deep copy the model\n",
    "        if val_acc > best_acc:\n",
    "            best_train_acc = train_acc\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            path = \"best_model.pt\"\n",
    "            torch.save(best_model_wts, path)\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}s'.format(time_elapsed))\n",
    "    print('Best Train Acc: {:4f}'.format(best_train_acc))\n",
    "    print('Best Val Acc: {:4f}'.format(best_val_acc))\n",
    "    \n",
    "    \n",
    "#     stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "    layers = [module for module in model.modules() if type(module) != nn.Sequential]\n",
    "    write_results([layers, num_epochs, minibatch_size, learning_rate, optimizer.state_dict, scheduler.state_dict, best_train_acc, best_val_acc])\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "Training Loss: 1.2393 Acc: 0.4511\n",
      "Validation Loss: 1.1129 Acc: 0.5212\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "Training Loss: 1.1524 Acc: 0.4947\n",
      "Validation Loss: 1.1201 Acc: 0.5212\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "Training Loss: 1.1351 Acc: 0.4977\n",
      "Validation Loss: 1.1068 Acc: 0.5212\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "Training Loss: 1.1266 Acc: 0.4991\n",
      "Validation Loss: 1.1048 Acc: 0.5226\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "Training Loss: 1.1206 Acc: 0.5028\n",
      "Validation Loss: 1.0874 Acc: 0.5279\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "Training Loss: 1.0986 Acc: 0.5157\n",
      "Validation Loss: 1.0395 Acc: 0.5770\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "Training Loss: 1.0284 Acc: 0.5556\n",
      "Validation Loss: 0.8821 Acc: 0.6394\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "Training Loss: 0.9207 Acc: 0.5927\n",
      "Validation Loss: 0.8096 Acc: 0.6474\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "Training Loss: 0.8822 Acc: 0.6060\n",
      "Validation Loss: 0.7389 Acc: 0.6687\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "Training Loss: 0.8437 Acc: 0.6195\n",
      "Validation Loss: 0.7339 Acc: 0.6713\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "Training Loss: 0.8129 Acc: 0.6325\n",
      "Validation Loss: 0.7366 Acc: 0.6414\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "Training Loss: 0.7975 Acc: 0.6344\n",
      "Validation Loss: 0.7024 Acc: 0.6614\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "Training Loss: 0.7752 Acc: 0.6392\n",
      "Validation Loss: 0.6813 Acc: 0.6813\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "Training Loss: 0.7587 Acc: 0.6468\n",
      "Validation Loss: 0.6757 Acc: 0.6846\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "Training Loss: 0.7500 Acc: 0.6495\n",
      "Validation Loss: 0.6757 Acc: 0.6713\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "Training Loss: 0.7355 Acc: 0.6567\n",
      "Validation Loss: 0.6442 Acc: 0.6959\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "Training Loss: 0.7321 Acc: 0.6662\n",
      "Validation Loss: 0.6490 Acc: 0.6999\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "Training Loss: 0.7189 Acc: 0.6649\n",
      "Validation Loss: 0.6309 Acc: 0.6886\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "Training Loss: 0.7048 Acc: 0.6692\n",
      "Validation Loss: 0.6227 Acc: 0.6946\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "Training Loss: 0.7054 Acc: 0.6712\n",
      "Validation Loss: 0.6127 Acc: 0.7085\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "Training Loss: 0.7009 Acc: 0.6812\n",
      "Validation Loss: 0.6157 Acc: 0.7078\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "Training Loss: 0.6856 Acc: 0.6812\n",
      "Validation Loss: 0.5972 Acc: 0.7105\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "Training Loss: 0.6852 Acc: 0.6818\n",
      "Validation Loss: 0.5937 Acc: 0.7151\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "Training Loss: 0.6782 Acc: 0.6889\n",
      "Validation Loss: 0.6159 Acc: 0.6912\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "Training Loss: 0.6699 Acc: 0.6973\n",
      "Validation Loss: 0.6030 Acc: 0.6999\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "Training Loss: 0.6694 Acc: 0.6883\n",
      "Validation Loss: 0.5832 Acc: 0.7158\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "Training Loss: 0.6593 Acc: 0.6928\n",
      "Validation Loss: 0.5790 Acc: 0.7125\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "Training Loss: 0.6461 Acc: 0.6969\n",
      "Validation Loss: 0.5709 Acc: 0.7171\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "Training Loss: 0.6421 Acc: 0.7045\n",
      "Validation Loss: 0.5661 Acc: 0.7112\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "Training Loss: 0.6378 Acc: 0.7027\n",
      "Validation Loss: 0.5868 Acc: 0.7085\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "Training Loss: 0.6504 Acc: 0.6986\n",
      "Validation Loss: 0.5644 Acc: 0.7198\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "Training Loss: 0.6593 Acc: 0.6936\n",
      "Validation Loss: 0.5706 Acc: 0.7297\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "Training Loss: 0.6290 Acc: 0.6999\n",
      "Validation Loss: 0.5679 Acc: 0.7317\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "Training Loss: 0.6273 Acc: 0.7116\n",
      "Validation Loss: 0.5607 Acc: 0.7404\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "Training Loss: 0.6326 Acc: 0.7136\n",
      "Validation Loss: 0.5621 Acc: 0.7331\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "Training Loss: 0.6270 Acc: 0.7155\n",
      "Validation Loss: 0.5469 Acc: 0.7384\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "Training Loss: 0.6408 Acc: 0.7161\n",
      "Validation Loss: 0.5556 Acc: 0.7437\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "Training Loss: 0.6248 Acc: 0.7150\n",
      "Validation Loss: 0.5525 Acc: 0.7430\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "Training Loss: 0.6203 Acc: 0.7276\n",
      "Validation Loss: 0.5596 Acc: 0.7430\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "Training Loss: 0.6230 Acc: 0.7205\n",
      "Validation Loss: 0.5344 Acc: 0.7656\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "Training Loss: 0.6131 Acc: 0.7184\n",
      "Validation Loss: 0.5451 Acc: 0.7517\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "Training Loss: 0.6223 Acc: 0.7165\n",
      "Validation Loss: 0.5632 Acc: 0.7543\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "Training Loss: 0.6169 Acc: 0.7219\n",
      "Validation Loss: 0.5734 Acc: 0.7390\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "Training Loss: 0.6177 Acc: 0.7237\n",
      "Validation Loss: 0.5302 Acc: 0.7749\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "Training Loss: 0.6042 Acc: 0.7367\n",
      "Validation Loss: 0.5365 Acc: 0.7576\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "Training Loss: 0.6118 Acc: 0.7354\n",
      "Validation Loss: 0.5652 Acc: 0.7138\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "Training Loss: 0.5950 Acc: 0.7402\n",
      "Validation Loss: 0.5431 Acc: 0.7444\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "Training Loss: 0.5973 Acc: 0.7369\n",
      "Validation Loss: 0.5098 Acc: 0.7736\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "Training Loss: 0.5956 Acc: 0.7342\n",
      "Validation Loss: 0.5363 Acc: 0.7430\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "Training Loss: 0.6040 Acc: 0.7276\n",
      "Validation Loss: 0.5409 Acc: 0.7477\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "Training Loss: 0.5907 Acc: 0.7319\n",
      "Validation Loss: 0.5333 Acc: 0.7437\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "Training Loss: 0.5869 Acc: 0.7390\n",
      "Validation Loss: 0.5394 Acc: 0.7397\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "Training Loss: 0.5948 Acc: 0.7370\n",
      "Validation Loss: 0.5305 Acc: 0.7497\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "Training Loss: 0.5956 Acc: 0.7386\n",
      "Validation Loss: 0.5267 Acc: 0.7756\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "Training Loss: 0.6004 Acc: 0.7443\n",
      "Validation Loss: 0.5307 Acc: 0.7596\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "Training Loss: 0.5821 Acc: 0.7439\n",
      "Validation Loss: 0.5195 Acc: 0.7643\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "Training Loss: 0.5859 Acc: 0.7507\n",
      "Validation Loss: 0.5663 Acc: 0.7371\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "Training Loss: 0.5871 Acc: 0.7453\n",
      "Validation Loss: 0.5384 Acc: 0.7603\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "Training Loss: 0.5836 Acc: 0.7449\n",
      "Validation Loss: 0.5279 Acc: 0.7590\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "Training Loss: 0.5479 Acc: 0.7642\n",
      "Validation Loss: 0.5151 Acc: 0.7782\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "Training Loss: 0.5508 Acc: 0.7646\n",
      "Validation Loss: 0.5062 Acc: 0.7862\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "Training Loss: 0.5330 Acc: 0.7763\n",
      "Validation Loss: 0.4973 Acc: 0.7895\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "Training Loss: 0.5260 Acc: 0.7774\n",
      "Validation Loss: 0.4950 Acc: 0.7822\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "Training Loss: 0.5154 Acc: 0.7824\n",
      "Validation Loss: 0.4924 Acc: 0.7822\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "Training Loss: 0.5328 Acc: 0.7745\n",
      "Validation Loss: 0.4862 Acc: 0.7908\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "Training Loss: 0.5087 Acc: 0.7850\n",
      "Validation Loss: 0.4874 Acc: 0.7902\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "Training Loss: 0.5203 Acc: 0.7809\n",
      "Validation Loss: 0.4839 Acc: 0.7975\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "Training Loss: 0.5020 Acc: 0.7908\n",
      "Validation Loss: 0.4863 Acc: 0.7888\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "Training Loss: 0.4988 Acc: 0.7937\n",
      "Validation Loss: 0.4834 Acc: 0.7981\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "Training Loss: 0.4931 Acc: 0.7949\n",
      "Validation Loss: 0.4754 Acc: 0.8068\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "Training Loss: 0.4820 Acc: 0.7981\n",
      "Validation Loss: 0.4732 Acc: 0.8054\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "Training Loss: 0.5089 Acc: 0.7835\n",
      "Validation Loss: 0.4803 Acc: 0.8001\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "Training Loss: 0.4806 Acc: 0.7934\n",
      "Validation Loss: 0.4716 Acc: 0.8008\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "Training Loss: 0.4816 Acc: 0.7945\n",
      "Validation Loss: 0.4702 Acc: 0.8041\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "Training Loss: 0.4906 Acc: 0.7941\n",
      "Validation Loss: 0.4710 Acc: 0.7988\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "Training Loss: 0.4816 Acc: 0.8017\n",
      "Validation Loss: 0.4665 Acc: 0.8035\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "Training Loss: 0.4745 Acc: 0.7967\n",
      "Validation Loss: 0.4752 Acc: 0.7988\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "Training Loss: 0.4729 Acc: 0.7940\n",
      "Validation Loss: 0.4716 Acc: 0.8054\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "Training Loss: 0.4735 Acc: 0.8014\n",
      "Validation Loss: 0.4648 Acc: 0.8094\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "Training Loss: 0.4777 Acc: 0.7993\n",
      "Validation Loss: 0.4641 Acc: 0.8074\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "Training Loss: 0.4739 Acc: 0.8017\n",
      "Validation Loss: 0.4640 Acc: 0.8041\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "Training Loss: 0.4586 Acc: 0.8091\n",
      "Validation Loss: 0.4606 Acc: 0.8108\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "Training Loss: 0.4764 Acc: 0.8022\n",
      "Validation Loss: 0.4633 Acc: 0.8074\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "Training Loss: 0.4586 Acc: 0.8065\n",
      "Validation Loss: 0.4614 Acc: 0.8134\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "Training Loss: 0.4773 Acc: 0.8033\n",
      "Validation Loss: 0.4637 Acc: 0.8094\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "Training Loss: 0.4624 Acc: 0.8063\n",
      "Validation Loss: 0.4592 Acc: 0.8161\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "Training Loss: 0.4577 Acc: 0.8074\n",
      "Validation Loss: 0.4646 Acc: 0.8101\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "Training Loss: 0.4619 Acc: 0.8084\n",
      "Validation Loss: 0.4594 Acc: 0.8108\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4473 Acc: 0.8151\n",
      "Validation Loss: 0.4631 Acc: 0.8081\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "Training Loss: 0.4516 Acc: 0.8120\n",
      "Validation Loss: 0.4600 Acc: 0.8114\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "Training Loss: 0.4413 Acc: 0.8138\n",
      "Validation Loss: 0.4600 Acc: 0.8114\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "Training Loss: 0.4554 Acc: 0.8098\n",
      "Validation Loss: 0.4643 Acc: 0.8141\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "Training Loss: 0.4429 Acc: 0.8111\n",
      "Validation Loss: 0.4559 Acc: 0.8114\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "Training Loss: 0.4453 Acc: 0.8224\n",
      "Validation Loss: 0.4551 Acc: 0.8141\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "Training Loss: 0.4373 Acc: 0.8184\n",
      "Validation Loss: 0.4580 Acc: 0.8181\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "Training Loss: 0.4380 Acc: 0.8213\n",
      "Validation Loss: 0.4587 Acc: 0.8108\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "Training Loss: 0.4301 Acc: 0.8234\n",
      "Validation Loss: 0.4578 Acc: 0.8141\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "Training Loss: 0.4277 Acc: 0.8227\n",
      "Validation Loss: 0.4585 Acc: 0.8141\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "Training Loss: 0.4393 Acc: 0.8217\n",
      "Validation Loss: 0.4633 Acc: 0.8081\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "Training Loss: 0.4366 Acc: 0.8251\n",
      "Validation Loss: 0.4543 Acc: 0.8161\n",
      "\n",
      "Training complete in 541s\n",
      "Best Train Acc: 0.825140\n",
      "Best Val Acc: 0.816069\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop_layer = nn.Dropout(p=0.55)\n",
    "        self.fc1 = nn.Linear(29, 800)\n",
    "        self.fc2 = nn.Linear(800, 600)\n",
    "        self.fc3 = nn.Linear(600, 300)\n",
    "        self.fc4 = nn.Linear(300, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "n_epochs = 100\n",
    "minibatch_size = 16\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "final_mode = train_model(X_train, y_train, X_val, y_val, model, criterion, optimizer, scheduler, minibatch_size, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layers</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>BatchSize</th>\n",
       "      <th>LearningRate</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Scheduler</th>\n",
       "      <th>TrainAcc</th>\n",
       "      <th>ValAcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.485487</td>\n",
       "      <td>0.505976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.861423</td>\n",
       "      <td>0.822709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.880384</td>\n",
       "      <td>0.802125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.45, inplace...</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.787336</td>\n",
       "      <td>0.789509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.45, inplace...</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.676381</td>\n",
       "      <td>0.711819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.25, inplace...</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.896887</td>\n",
       "      <td>0.839309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.15, inplace...</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.938904</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.2, inplace=...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.914597</td>\n",
       "      <td>0.839973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.25, inplace...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.884021</td>\n",
       "      <td>0.838645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.473524</td>\n",
       "      <td>0.477424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.811421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method _LRScheduler.state_dict of &lt;torc...</td>\n",
       "      <td>0.890061</td>\n",
       "      <td>0.841301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.35, inplace...</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.806845</td>\n",
       "      <td>0.817397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.35, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.890881</td>\n",
       "      <td>0.843958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.902037</td>\n",
       "      <td>0.851262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.887757</td>\n",
       "      <td>0.851262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.745669</td>\n",
       "      <td>0.709163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.892439</td>\n",
       "      <td>0.832669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.45, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.892205</td>\n",
       "      <td>0.843293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.55, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.830173</td>\n",
       "      <td>0.819389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.55, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of AdamW (\\...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.829588</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.55, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adamax (...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.818118</td>\n",
       "      <td>0.828021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.55, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adamax (...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.815660</td>\n",
       "      <td>0.803453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.55, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of RMSprop ...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.822566</td>\n",
       "      <td>0.805445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[Net(\\n  (drop_layer): Dropout(p=0.55, inplace...</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>&lt;bound method Optimizer.state_dict of Adam (\\n...</td>\n",
       "      <td>&lt;bound method ReduceLROnPlateau.state_dict of ...</td>\n",
       "      <td>0.825140</td>\n",
       "      <td>0.816069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Layers  Epochs  BatchSize  \\\n",
       "0   [Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...       1         16   \n",
       "1   [Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...      50         16   \n",
       "2   [Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...      50         16   \n",
       "3   [Net(\\n  (drop_layer): Dropout(p=0.45, inplace...      50         16   \n",
       "4   [Net(\\n  (drop_layer): Dropout(p=0.45, inplace...      50         16   \n",
       "5   [Net(\\n  (drop_layer): Dropout(p=0.25, inplace...      50         16   \n",
       "6   [Net(\\n  (drop_layer): Dropout(p=0.15, inplace...      50         16   \n",
       "7   [Net(\\n  (drop_layer): Dropout(p=0.2, inplace=...      50          8   \n",
       "8   [Net(\\n  (drop_layer): Dropout(p=0.25, inplace...      50          8   \n",
       "9   [Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...      50          8   \n",
       "10  [Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...      50          8   \n",
       "11  [Net(\\n  (drop_layer): Dropout(p=0.3, inplace=...     100          4   \n",
       "12  [Net(\\n  (drop_layer): Dropout(p=0.35, inplace...      50          4   \n",
       "13  [Net(\\n  (drop_layer): Dropout(p=0.35, inplace...     100          4   \n",
       "14  [Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...     100         16   \n",
       "15  [Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...     100         16   \n",
       "16  [Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...     100         16   \n",
       "17  [Net(\\n  (drop_layer): Dropout(p=0.4, inplace=...     100         16   \n",
       "18  [Net(\\n  (drop_layer): Dropout(p=0.45, inplace...     100         16   \n",
       "19  [Net(\\n  (drop_layer): Dropout(p=0.55, inplace...     100         16   \n",
       "20  [Net(\\n  (drop_layer): Dropout(p=0.55, inplace...     100         16   \n",
       "21  [Net(\\n  (drop_layer): Dropout(p=0.55, inplace...     100         16   \n",
       "22  [Net(\\n  (drop_layer): Dropout(p=0.55, inplace...     100         16   \n",
       "23  [Net(\\n  (drop_layer): Dropout(p=0.55, inplace...     100         16   \n",
       "24  [Net(\\n  (drop_layer): Dropout(p=0.55, inplace...     100         16   \n",
       "\n",
       "    LearningRate                                          Optimizer  \\\n",
       "0         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "1         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "2         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "3         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "4         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "5         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "6         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "7         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "8         0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "9         0.0100  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "10        0.0001  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "11        0.0001  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "12        0.0001  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "13        0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "14        0.0015  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "15        0.0015  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "16        0.0015  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "17        0.0015  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "18        0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "19        0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "20        0.0010  <bound method Optimizer.state_dict of AdamW (\\...   \n",
       "21        0.0010  <bound method Optimizer.state_dict of Adamax (...   \n",
       "22        0.0010  <bound method Optimizer.state_dict of Adamax (...   \n",
       "23        0.0010  <bound method Optimizer.state_dict of RMSprop ...   \n",
       "24        0.0010  <bound method Optimizer.state_dict of Adam (\\n...   \n",
       "\n",
       "                                            Scheduler  TrainAcc    ValAcc  \n",
       "0   <bound method _LRScheduler.state_dict of <torc...  0.485487  0.505976  \n",
       "1   <bound method _LRScheduler.state_dict of <torc...  0.861423  0.822709  \n",
       "2   <bound method _LRScheduler.state_dict of <torc...  0.880384  0.802125  \n",
       "3   <bound method _LRScheduler.state_dict of <torc...  0.787336  0.789509  \n",
       "4   <bound method _LRScheduler.state_dict of <torc...  0.676381  0.711819  \n",
       "5   <bound method _LRScheduler.state_dict of <torc...  0.896887  0.839309  \n",
       "6   <bound method _LRScheduler.state_dict of <torc...  0.938904  0.833333  \n",
       "7   <bound method _LRScheduler.state_dict of <torc...  0.914597  0.839973  \n",
       "8   <bound method _LRScheduler.state_dict of <torc...  0.884021  0.838645  \n",
       "9   <bound method _LRScheduler.state_dict of <torc...  0.473524  0.477424  \n",
       "10  <bound method _LRScheduler.state_dict of <torc...  0.806818  0.811421  \n",
       "11  <bound method _LRScheduler.state_dict of <torc...  0.890061  0.841301  \n",
       "12  <bound method ReduceLROnPlateau.state_dict of ...  0.806845  0.817397  \n",
       "13  <bound method ReduceLROnPlateau.state_dict of ...  0.890881  0.843958  \n",
       "14  <bound method ReduceLROnPlateau.state_dict of ...  0.902037  0.851262  \n",
       "15  <bound method ReduceLROnPlateau.state_dict of ...  0.887757  0.851262  \n",
       "16  <bound method ReduceLROnPlateau.state_dict of ...  0.745669  0.709163  \n",
       "17  <bound method ReduceLROnPlateau.state_dict of ...  0.892439  0.832669  \n",
       "18  <bound method ReduceLROnPlateau.state_dict of ...  0.892205  0.843293  \n",
       "19  <bound method ReduceLROnPlateau.state_dict of ...  0.830173  0.819389  \n",
       "20  <bound method ReduceLROnPlateau.state_dict of ...  0.829588  0.833333  \n",
       "21  <bound method ReduceLROnPlateau.state_dict of ...  0.818118  0.828021  \n",
       "22  <bound method ReduceLROnPlateau.state_dict of ...  0.815660  0.803453  \n",
       "23  <bound method ReduceLROnPlateau.state_dict of ...  0.822566  0.805445  \n",
       "24  <bound method ReduceLROnPlateau.state_dict of ...  0.825140  0.816069  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('params_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Train Acc: 0.754, Val Acc: 0.722, Test Acc: 0.706\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='rbf')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "svc = svc.fit(X_train, y_train)\n",
    "\n",
    "train_pred = svc.predict(X_train)\n",
    "val_pred = svc.predict(X_val)\n",
    "test_pred = svc.predict(X_test)\n",
    "\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print (\"SVM Train Acc: {0:.3}, Val Acc: {1:.3}, Test Acc: {2:.3}\".format(train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
